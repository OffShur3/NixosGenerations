{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OffShur3/NixosGenerations/blob/main/Delivery%20note%20with%20OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nota de entrega con OCR\n",
        "En este programa nos genera notas de entrega para todos los consumos pendientes de las instalaciones, tiene un modo de reconocimiento de texto dentro de las fotos para poder detectar en caso de que haya algun serie mal escrito"
      ],
      "metadata": {
        "id": "vQgz0ijO-kW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importacion de librerias\n",
        "Esto es lo necesario para que el programa funcione, sin contar con el reconocimiento de OCR que aparece luego en caso de que se necesite"
      ],
      "metadata": {
        "id": "yFGYWBJJQS32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importacion de librerias\n",
        "import csv\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import pprint\n",
        "import cv2\n",
        "from time import sleep\n",
        "from datetime import date, datetime\n",
        "from urllib.parse import urlencode\n",
        "from collections import defaultdict\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.utils import dict_from_cookiejar\n",
        "\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "lPdoU0KzB2G0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Variables modificables**\n",
        "\n",
        "* `planilla_anual`: URL de la planilla de Google Sheets que contiene los datos de consumo. Es un recurso vital para el funcionamiento del script.\n",
        "* **APIs del ERP:** Estas son las claves (`API_KEY` y `API_SECRET`) que se generan en Frappe. Son esenciales para la autenticaci√≥n y el intercambio de datos con el servidor.\n",
        "* `FRAPPE_API`: Es el dominio o la URL donde est√° alojado el servidor de Frappe. El script obtiene esta URL de una celda espec√≠fica de otra planilla de Google Sheets.\n",
        "* `gpu_o_cpu`: Variable para seleccionar si el proceso de OCR se ejecutar√° en una GPU o en una CPU. Por defecto, est√° configurada para usar la CPU.\n",
        "* **Otras variables:** El resto de las variables (`archivo_crudo`, `archivo_Series`, etc.) son nombres de archivos temporales que no necesitan ser modificados. Los prefijos v√°lidos (`prefijos_validos`) se utilizan para validar los n√∫meros de serie detectados por el OCR."
      ],
      "metadata": {
        "id": "1J6kHoXaQrcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ocr con gpu o cpu?\n",
        "gpu_o_cpu = \"C\" #input(\"Para hacer el OCR usamos [G]pu o [C]pu?\")\n",
        "\n",
        "# === ~ Variables ===\n",
        "\n",
        "#crudo\n",
        "planilla_anual = userdata.get('planilla_anual')\n",
        "# URL Apps Script del Forms\n",
        "FormsAppscript = userdata.get('FormsAppscript')\n",
        "\n",
        "# === üîê API ERP ===\n",
        "API_KEY = userdata.get('API_KEY')\n",
        "API_SECRET = userdata.get('API_SECRET')\n",
        "FRAPPE_API_crudo = requests.get(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRbnN152KxnzZqA3GA0L0LQJCF9uGRNIXsydi_FB1EveJ_aVSD_5rjHtTcfoGyctN8ZPou5R7Eg-QQ8/pub?gid=1381182595&single=true&output=csv\").text.splitlines()[1]\n",
        "FRAPPE_API = FRAPPE_API_crudo+\"/api/resource\"\n",
        "\n",
        "\n",
        "# Nombre de los archivos a generar\n",
        "archivo_crudo = \"crudo.csv\" #para el crudo de las OT\n",
        "archivo_Series = \"series_frappe.csv\" #para el listado de series de frappe\n",
        "archivo_productos = \"productos.csv\" #para el listado de productos\n",
        "archivo_ONT = \"columnas_extraidasONT.csv\" #columnas de extraccion de datos ONT\n",
        "archivo_DECO = \"columnas_extraidasDeco.csv\" #columnas de extraccion de datos Decos\n",
        "archivo_consumibles = \"reporte_consumibles.csv\" #cables usados\n",
        "path_viejos=\"url_serieViejo.csv\"\n",
        "path_nuevos=\"series_validas_detectadas.csv\"\n",
        "prefijos_validos = ['4857', 'HP40', 'HP44', 'FZ32', '534D', 'ALCL']"
      ],
      "metadata": {
        "id": "BFhObpzk1mcx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nota de entrega"
      ],
      "metadata": {
        "id": "3buKfC34wHHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def install_other_dependencies():\n",
        "    import subprocess\n",
        "\n",
        "    # --- 1. Verificaci√≥n del entorno Colab ---\n",
        "    # El m√≥dulo google.colab solo existe en ese entorno\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "        print(\"Detectado: Google Colab. Se ejecutar√°n comandos de Colab.\")\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "        print(\"Detectado: Entorno local.\")\n",
        "\n",
        "    # --- 2. L√≥gica condicional ---\n",
        "    if IN_COLAB:\n",
        "        # C√≥digo espec√≠fico para Google Colab\n",
        "        print(\"\\n--- Instalando dependencias con comandos de Colab ---\")\n",
        "        !pip install pytesseract\n",
        "        !apt-get install -y tesseract-ocr libtesseract-dev\n",
        "        !pip install paddleocr\n",
        "        # Puedes a√±adir la l√≥gica para PaddlePaddle aqu√≠\n",
        "        # !pip install paddlepaddle-gpu o !pip install paddlepaddle\n",
        "    else:\n",
        "        # C√≥digo espec√≠fico para un entorno local\n",
        "        try:\n",
        "            # --- Si es Linux, ejecuta los comandos con subprocess ---\n",
        "            print(\"\\n--- Detectado: Linux. Instalando dependencias con apt y pip ---\")\n",
        "\n",
        "            # Comandos de apt-get\n",
        "            print(\"Ejecutando apt-get update...\")\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"update\"], check=True)\n",
        "\n",
        "            print(\"Ejecutando apt-get install tesseract-ocr...\")\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"tesseract-ocr\", \"libtesseract-dev\"], check=True)\n",
        "\n",
        "            # Comandos de pip\n",
        "            print(\"Instalando librer√≠as de Python con pip...\")\n",
        "            subprocess.run([\"pip\", \"install\", \"pytesseract\"], check=True)\n",
        "            if gpu_o_cpu == \"G\":\n",
        "                subprocess.run([\"pip\", \"install\", \"paddlepaddle-gpu\"], check=True)\n",
        "            else:\n",
        "                subprocess.run([\"pip\", \"install\", \"paddlepaddle\"], check=True)\n",
        "\n",
        "        except:\n",
        "            print(\"\\n--- Sistema operativo no compatible con este script de instalaci√≥n. ---\")\n",
        "            print(\"Por favor, instala las dependencias manualmente.\")"
      ],
      "metadata": {
        "id": "w2eBwyMBti4s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funciones de limpieza y estado**\n",
        "\n",
        "* `borrar_archivos_csv()`: Esta funci√≥n elimina los archivos CSV temporales que se crean durante la ejecuci√≥n del script. Su prop√≥sito es mantener el entorno limpio y evitar que se usen datos de ejecuciones anteriores.\n",
        "\n",
        "* `hay_que_revisar(only_boolean=False)`: Verifica si hay equipos pendientes de transferencia o que no fueron recibidos. Lee los archivos `ONT.csv` y `DECO.csv` para determinar su estado. La funci√≥n puede devolver un valor booleano (`True` o `False`) si se especifica, o imprimir mensajes detallados sobre los equipos que requieren atenci√≥n."
      ],
      "metadata": {
        "id": "danUXFUc0MZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def borrar_archivos_csv():\n",
        "    for archivo in [archivo_crudo, archivo_Series, archivo_productos, archivo_ONT, archivo_DECO, archivo_consumibles]:\n",
        "        if os.path.exists(archivo):\n",
        "            os.remove(archivo)\n",
        "            print(f\"üßπ Archivo eliminado: {archivo}\")\n",
        "\n",
        "def hay_que_revisar(only_boolean=False):\n",
        "    def tiene_pendientes(archivo):\n",
        "        with open(archivo, newline='', encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for fila in reader:\n",
        "                if fila.get(\"No recibido\", \"\").strip(): #fila.get(\"Transferencia?\", \"\").strip(): # si tenia transferencias me hacia OCR\n",
        "                    print(fila)\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    pendientes_ont = tiene_pendientes(archivo_ONT)\n",
        "    pendientes_deco = tiene_pendientes(archivo_DECO)\n",
        "\n",
        "    if only_boolean:\n",
        "        return pendientes_ont or pendientes_deco\n",
        "\n",
        "    hay_pendientes = False\n",
        "\n",
        "    if pendientes_ont:\n",
        "        print(f\"‚ö†Ô∏è A√∫n hay equipos en '{archivo_ONT}' que requieren transferencia o no fueron recibidos.\")\n",
        "        hay_pendientes = True\n",
        "\n",
        "    if pendientes_deco:\n",
        "        print(f\"‚ö†Ô∏è A√∫n hay equipos en '{archivo_DECO}' que requieren transferencia o no fueron recibidos.\")\n",
        "        hay_pendientes =\n",
        "\n",
        "    if pendientes_ont or pendientes_deco:\n",
        "      print(\"üö´ Revisar equipos no recibidos\")\n",
        "\n",
        "    return hay_pendientes"
      ],
      "metadata": {
        "id": "c66so-4rZcYU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funciones de comunicaci√≥n con Google Sheets, Forms y Frappe**\n",
        "\n",
        "* `abrirFotosNoRecibidos()`: Esta funci√≥n, marcada para uso local y no en Colab, extrae las URLs de las fotos de los equipos que no se han recibido. Su prop√≥sito es preparar los enlaces para su posterior descarga o visualizaci√≥n.\n",
        "\n",
        "* `enviarOTForms(ot_number)`: Env√≠a un n√∫mero de Orden de Trabajo (`ot_number`) a una aplicaci√≥n web de Google Apps Script. Esto se utiliza para registrar o notificar la finalizaci√≥n de una OT.\n",
        "\n",
        "* `ultimaOTconsumida()`: Lee la √∫ltima fila del archivo `crudo.csv` para encontrar el n√∫mero de la √∫ltima Orden de Trabajo procesada y luego llama a `enviarOTForms()` para enviar este n√∫mero a la aplicaci√≥n de Google Forms.\n",
        "\n",
        "* `agregar_comentario()`: Se conecta a la API de Frappe para a√±adir un comentario a un documento espec√≠fico. Recibe el tipo y nombre del documento, as√≠ como el contenido del comentario, y maneja de manera robusta los errores de conexi√≥n o de la API."
      ],
      "metadata": {
        "id": "ovJroQyw0pWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abrirFotosNoRecibidos(): # sin usar si es en colab\n",
        "    archivos = [archivo_ONT, archivo_DECO]\n",
        "    campo_fotos = \"foto (no recibido)\"\n",
        "    urls = set()\n",
        "\n",
        "    for archivo in archivos:\n",
        "        try:\n",
        "            with open(archivo, newline='', encoding=\"utf-8\") as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                for fila in reader:\n",
        "                    fotos_raw = fila.get(campo_fotos, \"\").strip()\n",
        "                    if fotos_raw:\n",
        "                        for link in fotos_raw.split(\"\\n\"):\n",
        "                            url = link.strip()\n",
        "                            if url.startswith(\"http\"):\n",
        "                                urls.add(url)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è No se pudo procesar {archivo}: {e}\")\n",
        "\n",
        "def enviarOTForms(ot_number):\n",
        "    \"\"\"\n",
        "    Env√≠a un n√∫mero de OT a la Web App de Google Apps Script.\n",
        "\n",
        "    Args:\n",
        "        ot_number (str): El n√∫mero de OT que se enviar√°.\n",
        "\n",
        "    Returns:\n",
        "        requests.Response: El objeto de respuesta de la petici√≥n.\n",
        "    \"\"\"\n",
        "        # Datos que se enviar√°n en el cuerpo de la petici√≥n (formato JSON)\n",
        "    payload = {\n",
        "        \"otNumber\": ot_number\n",
        "    }\n",
        "\n",
        "    # Cabeceras para indicar que el contenido es JSON\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Realizar la petici√≥n POST\n",
        "        response = requests.post(FormsAppscript, data=json.dumps(payload), headers=headers)\n",
        "\n",
        "        # Verificar si la petici√≥n fue exitosa (c√≥digo de estado 200)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"Enviado a Forms\")\n",
        "        else:\n",
        "            print(f\"Error en la petici√≥n. C√≥digo de estado: {response.status_code}\")\n",
        "            print(f\"Respuesta del servidor: {response.text}\")\n",
        "\n",
        "        return response\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Se produjo un error al conectar con la Web App: {e}\")\n",
        "        return None\n",
        "\n",
        "def ultimaOTconsumida():\n",
        "    archivo = archivo_crudo\n",
        "    ultima_fila = None\n",
        "    with open(archivo, newline='', encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        for fila in reader:\n",
        "            ultima_fila = fila\n",
        "    if ultima_fila and len(ultima_fila) > 5:\n",
        "        print()\n",
        "        print()\n",
        "        print(f\"√öltima OT del {archivo}: {ultima_fila[5]}\")\n",
        "        enviarOTForms(ultima_fila[5])\n",
        "        print()\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"No se pudo leer la OT en la √∫ltima fila de '{archivo}'.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def agregar_comentario(doctype: str, docname: str, contenido: str, remitente: str = \"aguida@qpsrl.com.ar\") -> dict:\n",
        "    \"\"\"\n",
        "    Agrega un comentario a un documento en Frappe utilizando las API Keys y endpoint predefinidos.\n",
        "\n",
        "    Args:\n",
        "        doctype (str): El tipo de documento de referencia en Frappe (ej: \"Stock Entry\", \"Delivery Note\").\n",
        "        docname (str): El nombre o ID del documento en Frappe (ej: \"MAT-STE-2025-01263\").\n",
        "        contenido (str): El contenido del comentario, se espera en formato HTML (ej: \"<p>Mi comentario</p>\").\n",
        "        remitente (str, optional): Email del remitente. Por defecto es \"aguida@qpsrl.com.ar\".\n",
        "\n",
        "    Returns:\n",
        "        dict: Resultado de la operaci√≥n, indicando √©xito, errores y la respuesta de Frappe.\n",
        "    \"\"\"\n",
        "    if not API_KEY or not API_SECRET:\n",
        "        print(\"Error: API credentials (API_KEY or API_SECRET) are not set.\")\n",
        "        return {\"success\": False, \"error\": \"API credentials not found\"}\n",
        "\n",
        "    if not FRAPPE_API_crudo:\n",
        "        print(\"Error: Frappe API endpoint (FRAPPE_API_crudo) is not set. Check CSV fetch.\")\n",
        "        return {\"success\": False, \"error\": \"Frappe API endpoint not found\"}\n",
        "\n",
        "    try:\n",
        "        # Usar el m√©todo add_comment de Frappe para una integraci√≥n m√°s robusta\n",
        "        url = f\"{FRAPPE_API_crudo}/api/method/frappe.desk.form.utils.add_comment\"\n",
        "\n",
        "        payload = {\n",
        "            \"reference_doctype\": doctype,\n",
        "            \"reference_name\": docname,\n",
        "            \"content\": contenido,\n",
        "            \"comment_email\": remitente,\n",
        "            \"comment_by\": remitente.split(\"@\")[0] # Utiliza la parte antes del @ como nombre\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"token {API_KEY}:{API_SECRET}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Accept\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()  # Lanza una excepci√≥n para errores HTTP (4xx o 5xx)\n",
        "\n",
        "        response_data = response.json()\n",
        "\n",
        "        print(f\"üó®Ô∏è Comentario agregado a {docname} ({doctype}) usando add_comment\")\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"doctype\": doctype,\n",
        "            \"docname\": docname,\n",
        "            \"comment\": contenido,\n",
        "            \"response\": response_data\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # Captura errores espec√≠ficos de requests (conexi√≥n, HTTP, etc.)\n",
        "        error_message = f\"Request Error: {e}\"\n",
        "        if response.status_code:\n",
        "            error_message += f\" (HTTP {response.status_code}: {response.text})\"\n",
        "        print(f\"‚ùå Error al enviar comentario a Frappe: {error_message}\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": error_message,\n",
        "            \"doctype\": doctype,\n",
        "            \"docname\": docname\n",
        "        }\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Captura errores si la respuesta no es JSON v√°lido\n",
        "        error_message = f\"JSON Decode Error: {e}. Response text: {response.text}\"\n",
        "        print(f\"‚ùå Error al procesar la respuesta JSON: {error_message}\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": error_message,\n",
        "            \"doctype\": doctype,\n",
        "            \"docname\": docname\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # Captura cualquier otro error inesperado\n",
        "        print(f\"‚ùå Error inesperado en agregar_comentario: {e}\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e),\n",
        "            \"doctype\": doctype,\n",
        "            \"docname\": docname\n",
        "        }"
      ],
      "metadata": {
        "id": "co5BXNjx0vF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funciones de obtenci√≥n de datos y filtrado b√°sico**\n",
        "\n",
        "* `obtenerCrudo()`: Esta funci√≥n se encarga de descargar los datos directamente de una planilla de Google Sheets. Filtra las filas que ya han sido procesadas, a√±ade la etiqueta \" - QPS\" a los nombres de los t√©cnicos si es necesario, y guarda los datos limpios en un archivo CSV local para su posterior uso. Si no encuentra filas pendientes, termina el script para evitar procesamientos innecesarios.\n",
        "\n",
        "* `obtenerSeriesFrappe()`: Se comunica con la API de Frappe para obtener una lista de todos los n√∫meros de serie (`Serial No`) que se encuentran en el sistema. Los guarda en un archivo CSV (`Series.csv`) para poder comparar y validar los datos de la planilla con el inventario del ERP.\n",
        "\n",
        "* `obtenerProductos()`: Descarga un reporte de \"Stock Projected Qty\" desde la API de Frappe. Este reporte contiene informaci√≥n sobre la cantidad de art√≠culos en cada almac√©n. La funci√≥n procesa esta respuesta y guarda los datos en un archivo CSV (`productos.csv`), lo que es esencial para realizar verificaciones de stock antes de crear las notas de entrega."
      ],
      "metadata": {
        "id": "VdCfxzQcZi8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obtenerCrudo():\n",
        "    print(\"Descargando CSV...\")\n",
        "    resp = requests.get(planilla_anual)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    lineas = resp.content.decode(\"utf-8\").splitlines()\n",
        "\n",
        "    print(\"Filtrando filas que no hayan sido consumidas...\")\n",
        "    reader = csv.reader(lineas, delimiter=',')\n",
        "    filas_filtradas = []\n",
        "\n",
        "    for fila in reader:\n",
        "        if len(fila) > 25 and fila[4].strip() == \"\" and ( # ac√° en podes cambiar el de fila 4 a Si por si hay algun serie que qued√≥ en el limbo\n",
        "            fila[23].strip() == \"EXITOSA\" or\n",
        "            fila[23].strip() == \"CONTINGENCIA / SUSPENDIDA\"\n",
        "        ):\n",
        "            # Columna D (√≠ndice 3) es el t√©cnico. Le agregamos \" - QPS\" si no lo tiene.\n",
        "            tecnico = fila[3].strip()\n",
        "            if tecnico and not tecnico.endswith(\" - QPS\"):\n",
        "                fila[3] = f\"{tecnico} - QPS\"\n",
        "            filas_filtradas.append(fila)\n",
        "\n",
        "    if not filas_filtradas:\n",
        "        print(\"üö´ No hay filas pendientes de consumo. Terminando ejecuci√≥n.\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    with open(archivo_crudo, \"w\", newline='', encoding=\"utf-8\") as f_out:\n",
        "        writer = csv.writer(f_out, delimiter=',')\n",
        "        writer.writerows(filas_filtradas)\n",
        "\n",
        "    print(f\"‚úÖ Filtrado completo. {len(filas_filtradas)} filas guardadas en '{archivo_crudo}'\")\n",
        "\n",
        "\n",
        "def obtenerSeriesFrappe():\n",
        "    print(\"Consultando series en Frappe...\")\n",
        "\n",
        "    FRAPPE_API_serie = FRAPPE_API+\"/Serial%20No\"\n",
        "    HEADERS = {\n",
        "        \"Authorization\": f\"token {API_KEY}:{API_SECRET}\"\n",
        "    }\n",
        "\n",
        "    campos = [\n",
        "        \"name\", \"docstatus\", \"item_code\", \"item_name\",\n",
        "        \"warehouse\", \"creation\", \"modified\", \"_user_tags\"\n",
        "    ]\n",
        "\n",
        "    fields_param = json.dumps(campos)  # ‚úÖ Convierte a JSON v√°lido\n",
        "    url = f\"{FRAPPE_API_serie}?fields={fields_param}&limit_page_length=1000\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        data = response.json().get(\"data\", [])\n",
        "\n",
        "        with open(archivo_Series, \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"sr\"] + campos)  # encabezados\n",
        "\n",
        "            for i, fila in enumerate(data, 1):\n",
        "                writer.writerow([\n",
        "                    i,\n",
        "                    fila.get(\"name\", \"\"),\n",
        "                    fila.get(\"docstatus\", \"\"),\n",
        "                    fila.get(\"item_code\", \"\"),\n",
        "                    fila.get(\"item_name\", \"\"),\n",
        "                    fila.get(\"warehouse\", \"\"),\n",
        "                    fila.get(\"creation\", \"\"),\n",
        "                    fila.get(\"modified\", \"\"),\n",
        "                    fila.get(\"_user_tags\", \"\")\n",
        "                ])\n",
        "\n",
        "        print(f\"‚úÖ {len(data)} series guardadas en '{archivo_Series}'\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error al hacer la solicitud a Frappe: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def obtenerProductos():\n",
        "    \"\"\"Funci√≥n principal para obtener el stock proyectado\"\"\"\n",
        "    print(\"Iniciando descarga de stock proyectado...\")\n",
        "\n",
        "    # Configuraci√≥n de headers que funciona\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {API_KEY}:{API_SECRET}\",\n",
        "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
        "        \"ngrok-skip-browser-warning\": \"true\",\n",
        "        \"X-Frappe-CSRF-Token\": \"None\"\n",
        "    }\n",
        "\n",
        "    # Payload que funciona (seg√∫n pruebas)\n",
        "    payload = {\n",
        "        \"cmd\": \"frappe.desk.query_report.run\",\n",
        "        \"report_name\": \"Stock Projected Qty\",\n",
        "        \"filters\": json.dumps({\"company\": \"Quality Plus\", \"ignore_permissions\": 1}),\n",
        "        \"limit\": \"1000\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Enviar solicitud\n",
        "        response = requests.post(\n",
        "            f\"{FRAPPE_API}/api/method/frappe.desk.query_report.run\",\n",
        "            headers=headers,\n",
        "            data=urlencode(payload)\n",
        "        ) # Added the missing closing parenthesis here\n",
        "\n",
        "        # Verificar respuesta\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data.get(\"message\") or not isinstance(data[\"message\"].get(\"result\"), list):\n",
        "            raise Exception(\"La respuesta no contiene datos v√°lidos\")\n",
        "\n",
        "        resultados = data[\"message\"][\"result\"]\n",
        "        print(f\"Se obtuvieron {len(resultados)} registros de stock\")\n",
        "\n",
        "        # Columnas fijas basadas en el reporte Stock Projected Qty\n",
        "        columnas = [\n",
        "            \"item_code\", \"item_name\",\"warehouse\", \"actual_qty\"\n",
        "        ]\n",
        "\n",
        "        # Guardar en CSV\n",
        "        with open(archivo_productos, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=columnas, extrasaction='ignore')\n",
        "            writer.writeheader()\n",
        "\n",
        "            for row in resultados:\n",
        "                # Asegurarnos de que sea un diccionario\n",
        "                if isinstance(row, dict):\n",
        "                    writer.writerow(row)\n",
        "                else:\n",
        "                    # Si es una lista, convertir a diccionario con las columnas conocidas\n",
        "                    if isinstance(row, list) and len(row) == len(columnas):\n",
        "                        writer.writerow(dict(zip(columnas, row)))\n",
        "                    #else:\n",
        "                    #    print(f\"‚ö†Ô∏è Formato de fila no reconocido: {row}\")\n",
        "\n",
        "        print(f\"‚úÖ Datos guardados correctamente en {archivo_productos}\")\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"‚ùå Error en la conexi√≥n: {str(e)}\")\n",
        "        if hasattr(e, 'response') and e.response:\n",
        "            print(f\"Respuesta del servidor: {e.response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al procesar los datos: {str(e)}\")\n",
        "        if 'data' in locals() and data.get('message') and data['message'].get('result'):\n",
        "             print(f\"Tipo de datos recibidos: {type(data['message']['result'][0])}\")\n",
        "        else:\n",
        "             print(\"Tipo de datos recibidos: N/A\")\n",
        "\n",
        "\n",
        "    return False"
      ],
      "metadata": {
        "id": "gt82fPMtZpxt"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funciones de filtrado y categorizaci√≥n**\n",
        "\n",
        "* `extraer_columnas_ont()`: Esta funci√≥n lee los datos de la planilla (`crudo.csv`) y los de los n√∫meros de serie (`series_frappe.csv`) para procesar los equipos ONT (modems de fibra). Filtra las filas de \"instalaci√≥n, cambio de domicilio\" y determina si un equipo fue recibido, si requiere una transferencia de stock o si debe ser consumido en la nota de entrega. El resultado se guarda en `columnas_extraidasONT.csv`.\n",
        "\n",
        "* `extraer_columnas_deco()`: Similar a la funci√≥n anterior, pero se enfoca en los decodificadores. Procesa las filas del archivo `crudo.csv` que contienen series de Decos. Esta funci√≥n tambi√©n verifica el estado de cada Deco (si requiere transferencia, fue recibido o debe consumirse) y guarda la informaci√≥n en un archivo CSV espec√≠fico para Decos (`columnas_extraidasDeco.csv`).\n",
        "\n",
        "* `extraer_consumibles()`: Esta funci√≥n es responsable de calcular el consumo de consumibles (como cables y conectores) bas√°ndose en los datos de las OTs y el stock disponible. Carga la informaci√≥n de stock desde `productos.csv` y aplica una l√≥gica espec√≠fica para ciertos √≠tems, ajustando el consumo solicitado con el stock real disponible. Finalmente, genera un informe detallado de los consumos finales en `reporte_consumibles.csv`."
      ],
      "metadata": {
        "id": "uNt_SmYWZvGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_columnas_ont():\n",
        "    archivo_entrada = archivo_crudo\n",
        "    archivo_salida = archivo_ONT\n",
        "\n",
        "    encabezados = [\n",
        "        \"ONT (modem fibra)\",\n",
        "        \"foto\",\n",
        "        \"Carg√≥ OT\",\n",
        "        \"Origen equipo\",\n",
        "        \"Transferencia?\",\n",
        "        \"No recibido\",\n",
        "        \"foto (no recibido)\",\n",
        "        \"Consumir\"\n",
        "    ]\n",
        "\n",
        "    # Leer series_frappe.csv y construir un diccionario {serie: warehouse}\n",
        "    dic_ont_origen = {}\n",
        "    with open(archivo_Series, newline='', encoding=\"utf-8\") as f_series:\n",
        "        reader_series = csv.reader(f_series)\n",
        "        next(reader_series)  # saltar encabezado\n",
        "        for fila in reader_series:\n",
        "            if len(fila) > 5:\n",
        "                serie = fila[1].strip()\n",
        "                warehouse = fila[5].strip()\n",
        "                dic_ont_origen[serie] = warehouse\n",
        "\n",
        "    # Leer crudo.csv y procesar\n",
        "    with open(archivo_entrada, newline='', encoding=\"utf-8\") as f_in:\n",
        "        reader = csv.reader(f_in)\n",
        "        filas_salida = []\n",
        "\n",
        "        for fila in reader:\n",
        "            if len(fila) <= 9:\n",
        "                continue  # ignorar filas mal formateadas\n",
        "\n",
        "            # Verificar si en columna Z (√≠ndice 26 - 1 = 25) est√° \"INSTALACION CAMBIO DE DOMICILIO\"\n",
        "            if len(fila) > 25 and fila[25].strip().upper() == \"INSTALACION CAMBIO DE DOMICILIO\":\n",
        "                continue  # saltar esta fila\n",
        "\n",
        "            ont = fila[9].strip()\n",
        "            if not ont:\n",
        "                continue\n",
        "\n",
        "            foto = fila[8].strip()\n",
        "            cargo_ot_base = fila[3].strip()\n",
        "            cargo_ot = f\"{cargo_ot_base}\" if cargo_ot_base else \"\"\n",
        "\n",
        "            # Determinar origen equipo\n",
        "            if ont in dic_ont_origen and dic_ont_origen[ont].strip():\n",
        "                origen_equipo = dic_ont_origen[ont].strip()\n",
        "            elif ont not in dic_ont_origen:\n",
        "                origen_equipo = \"No\"\n",
        "            else:\n",
        "                origen_equipo = \"\"\n",
        "\n",
        "            # Transferencia (solo si origen_equipo no es \"No\")\n",
        "            if origen_equipo and origen_equipo != cargo_ot and origen_equipo != \"No\":\n",
        "                transferencia = f\"{origen_equipo}, {cargo_ot}\"\n",
        "            else:\n",
        "                transferencia = \"\"\n",
        "\n",
        "            no_recibido = \"\"\n",
        "            foto_no_recibido = \"\"\n",
        "\n",
        "            if origen_equipo == \"No\":\n",
        "                no_recibido = ont\n",
        "                foto_no_recibido = \"\\n\".join([f.strip() for f in foto.split(\",\") if f.strip()])\n",
        "\n",
        "            consumir = \"\"\n",
        "            if origen_equipo != \"No\" and origen_equipo == cargo_ot:\n",
        "                consumir = ont\n",
        "\n",
        "            filas_salida.append([\n",
        "                ont,\n",
        "                foto,\n",
        "                cargo_ot,\n",
        "                origen_equipo,\n",
        "                transferencia,\n",
        "                no_recibido,\n",
        "                foto_no_recibido,\n",
        "                consumir\n",
        "            ])\n",
        "\n",
        "    with open(archivo_salida, \"w\", newline='', encoding=\"utf-8\") as f_out:\n",
        "        writer = csv.writer(f_out)\n",
        "        writer.writerow(encabezados)\n",
        "        writer.writerows(filas_salida)\n",
        "\n",
        "    print(f\"‚úÖ Archivo '{archivo_salida}' generado con {len(filas_salida)} filas filtradas.\")\n",
        "\n",
        "\n",
        "def extraer_columnas_deco():\n",
        "    archivo_entrada = archivo_crudo\n",
        "    archivo_salida = archivo_DECO\n",
        "\n",
        "    encabezados = [\n",
        "        \"Deco\",\n",
        "        \"foto\",\n",
        "        \"Carg√≥ OT\",\n",
        "        \"Origen equipo\",\n",
        "        \"Transferencia?\",\n",
        "        \"No recibido\",\n",
        "        \"foto (no recibido)\",\n",
        "        \"Consumir\"\n",
        "    ]\n",
        "\n",
        "    # Leer series_frappe.csv y construir un diccionario {serie: warehouse}\n",
        "    dic_deco_origen = {}\n",
        "    with open(archivo_Series, newline='', encoding=\"utf-8\") as f_series:\n",
        "        reader_series = csv.reader(f_series)\n",
        "        next(reader_series)  # saltar encabezado\n",
        "        for fila in reader_series:\n",
        "            if len(fila) > 5:\n",
        "                serie = fila[1].strip()\n",
        "                warehouse = fila[5].strip()\n",
        "                dic_deco_origen[serie] = warehouse\n",
        "\n",
        "    # Leer crudo.csv y procesar\n",
        "    with open(archivo_entrada, newline='', encoding=\"utf-8\") as f_in:\n",
        "        reader = csv.reader(f_in)\n",
        "        filas_salida = []\n",
        "\n",
        "        for fila in reader:\n",
        "            # ‚ùó Ignorar si columna AB (√≠ndice 27) es \"INSTALACION CAMBIO DE DOMICILIO\"\n",
        "            if len(fila) > 27 and fila[27].strip().upper() == \"INSTALACION CAMBIO DE DOMICILIO\":\n",
        "                continue\n",
        "\n",
        "            if len(fila) > 10 and fila[10].strip() != \"\":\n",
        "                deco_series = fila[10].strip().split()  # Puede haber m√∫ltiples series separados por espacio\n",
        "                foto = fila[8].strip()\n",
        "                cargo_ot_base = fila[3].strip()\n",
        "                cargo_ot = f\"{cargo_ot_base}\" if cargo_ot_base else \"\"\n",
        "\n",
        "                for deco in deco_series:\n",
        "                    origen_equipo = \"No\" # esto es por si no se recibi√≥ el equipo\n",
        "                    if deco in dic_deco_origen and dic_deco_origen[deco].strip():\n",
        "                        origen_equipo = dic_deco_origen[deco].strip()\n",
        "                    elif deco not in dic_deco_origen:\n",
        "                        origen_equipo = \"No\"\n",
        "                    else:\n",
        "                        origen_equipo = \"\"\n",
        "\n",
        "                    # Transferencia (solo si origen_equipo no es \"No\")\n",
        "                    if origen_equipo and origen_equipo != cargo_ot and origen_equipo != \"No\":\n",
        "                        transferencia = f\"{origen_equipo}, {cargo_ot}\"\n",
        "                    else:\n",
        "                        transferencia = \"\"\n",
        "\n",
        "                    no_recibido = \"\"\n",
        "                    foto_no_recibido = \"\"\n",
        "                    if origen_equipo == \"No\":\n",
        "                        no_recibido = deco\n",
        "                        foto_no_recibido = \"\\n\".join([f.strip() for f in foto.split(\",\") if f.strip()])\n",
        "\n",
        "                    # Consumir: si no requiere transferencia y no tiene origen \"No\"\n",
        "                    consumir = \"\"\n",
        "                    if origen_equipo != \"No\": # and origen_equipo == cargo_ot: # si el equipo se recibi√≥ y el origen es el que lo carg√≥\n",
        "                        consumir = deco\n",
        "\n",
        "                    filas_salida.append([\n",
        "                        deco,\n",
        "                        foto,\n",
        "                        cargo_ot,\n",
        "                        origen_equipo,\n",
        "                        transferencia,\n",
        "                        no_recibido,\n",
        "                        foto_no_recibido,\n",
        "                        consumir\n",
        "                    ])\n",
        "\n",
        "    with open(archivo_salida, \"w\", newline='', encoding=\"utf-8\") as f_out:\n",
        "        writer = csv.writer(f_out)\n",
        "        writer.writerow(encabezados)\n",
        "        writer.writerows(filas_salida)\n",
        "\n",
        "    print(f\"‚úÖ Archivo '{archivo_salida}' generado con {len(filas_salida)} filas.\")\n",
        "\n",
        "def extraer_consumibles():\n",
        "    # Aseg√∫rate de que las variables globales sean accesibles\n",
        "    global archivo_consumibles, archivo_productos, archivo_crudo, archivo_DECO\n",
        "\n",
        "    archivo_salida = archivo_consumibles\n",
        "    item_code_fijo = \"20710003\" # Ejemplo: Cable UTP 1 metro\n",
        "    item_code_extra1 = \"110100372\" # Ejemplo: Conector SC/APC\n",
        "    item_code_extra2 = \"320500065\" # Ejemplo: Pigtail SC/APC\n",
        "\n",
        "    # √çtems que se consumen \"por completo seg√∫n stock total\" (independientemente de la cantidad solicitada)\n",
        "    ITEMS_CONSUMIR_TODO_STOCK = {\n",
        "        \"140110033\", \"170200005\", \"170200010\",\n",
        "        \"170300004\", \"20310001\", \"20710010\"\n",
        "    }\n",
        "\n",
        "    print(\"üõ†Ô∏è Iniciando extracci√≥n y c√°lculo de consumibles...\")\n",
        "\n",
        "    # 1. Cargar diccionario de productos y stock disponible\n",
        "    # productos: item_code -> item_name\n",
        "    # stock_disponible: item_code -> warehouse -> actual_qty (float)\n",
        "    productos = {}\n",
        "    stock_disponible = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    if os.path.exists(archivo_productos):\n",
        "        with open(archivo_productos, newline='', encoding=\"utf-8\") as f_stock:\n",
        "            reader = csv.DictReader(f_stock)\n",
        "            for row in reader:\n",
        "                codigo = row.get(\"item_code\", \"\").strip()\n",
        "                nombre = row.get(\"item_name\", \"\").strip()\n",
        "                almacen = row.get(\"warehouse\", \"\").strip()\n",
        "                try:\n",
        "                    stock = float(row.get(\"actual_qty\", 0))\n",
        "                except ValueError:\n",
        "                    stock = 0.0\n",
        "                    print(f\"‚ö†Ô∏è Stock no num√©rico para {codigo} en {almacen}. Asumiendo 0.\")\n",
        "\n",
        "                if codigo and almacen: # Asegurarse de que tengamos un c√≥digo y un almac√©n v√°lidos\n",
        "                    productos[codigo] = nombre\n",
        "                    stock_disponible[codigo][almacen] = stock\n",
        "    else:\n",
        "        print(f\"‚ùå Advertencia: Archivo '{archivo_productos}' no encontrado. No se aplicar√° control de stock para NING√öN √≠tem.\")\n",
        "        # Si el archivo de stock no existe, todos los √≠tems se tratar√°n como si no tuvieran stock para control,\n",
        "        # lo que significa que solo se consumir√°n si NO est√°n en ITEMS_CONSUMIR_TODO_STOCK (y por lo tanto, su stock ser√° 0).\n",
        "\n",
        "    # 2. Contar ocurrencias por (item_code, warehouse) de lo que se NECESITA consumir\n",
        "    # conteo_solicitado: item_code -> warehouse -> qty_solicitada (int)\n",
        "    conteo_solicitado = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Procesar archivo_crudo (columna N: cables, columna O: cantidad fija)\n",
        "    if os.path.exists(archivo_crudo):\n",
        "        with open(archivo_crudo, newline='', encoding=\"utf-8\") as f_crudo:\n",
        "            reader = csv.reader(f_crudo)\n",
        "            for fila in reader:\n",
        "                if len(fila) < 15: # Necesitamos hasta la columna O (√≠ndice 14)\n",
        "                    continue\n",
        "\n",
        "                campo_cable = fila[13].strip() # Columna N\n",
        "                campo_consumible = fila[14].strip() # Columna O\n",
        "                cargo_ot_base = fila[3].strip() # Columna D (T√©cnico / Almac√©n)\n",
        "                warehouse = cargo_ot_base if cargo_ot_base else \"Default Warehouse\"\n",
        "\n",
        "                # üîπ Cables (de columna N)\n",
        "                if \" - \" in campo_cable:\n",
        "                    item_code_cable = campo_cable.split(\" - \")[0].strip()\n",
        "                    if item_code_cable:\n",
        "                        conteo_solicitado[item_code_cable][warehouse] += 1\n",
        "\n",
        "                # üîπ Cantidades para item_code fijo (de columna O)\n",
        "                if campo_consumible:\n",
        "                    try:\n",
        "                        cantidad_fija = int(campo_consumible)\n",
        "                        if cantidad_fija > 0:\n",
        "                            conteo_solicitado[item_code_fijo][warehouse] += cantidad_fija\n",
        "                    except ValueError:\n",
        "                        print(f\"‚ö†Ô∏è Valor no num√©rico en columna O de '{archivo_crudo}' para {warehouse}: '{campo_consumible}'. Ignorado.\")\n",
        "    else:\n",
        "        print(f\"‚ùå Advertencia: Archivo '{archivo_crudo}' no encontrado. No se procesar√°n consumibles del crudo.\")\n",
        "\n",
        "    # Procesar serializados desde archivo_DECO\n",
        "    if os.path.exists(archivo_DECO):\n",
        "        with open(archivo_DECO, newline='', encoding=\"utf-8\") as f_deco:\n",
        "            reader = csv.DictReader(f_deco)\n",
        "            for fila in reader:\n",
        "                serie = fila.get(\"Consumir\", \"\").strip()\n",
        "                almacen = fila.get(\"Carg√≥ OT\", \"\").strip()\n",
        "\n",
        "                if serie and almacen:\n",
        "                    warehouse_deco = almacen\n",
        "                    conteo_solicitado[item_code_extra1][warehouse_deco] += 1\n",
        "                    conteo_solicitado[item_code_extra2][warehouse_deco] += 2\n",
        "    else:\n",
        "        print(f\"‚ùå Advertencia: Archivo '{archivo_DECO}' no encontrado. No se procesar√°n serializados de DECO.\")\n",
        "\n",
        "    # 3. Calcular consumos finales aplicando la l√≥gica de stock\n",
        "    # consumos_finales: item_code -> warehouse -> {'solicitada': X, 'real': Y}\n",
        "    consumos_finales = defaultdict(lambda: defaultdict(lambda: {'solicitada': 0, 'real': 0}))\n",
        "\n",
        "    # (L√≥gica original de procesamiento de √≠tems solicitados)\n",
        "    for item_code, almacenes_solicitados in conteo_solicitado.items():\n",
        "        for warehouse, qty_solicitada in almacenes_solicitados.items():\n",
        "            qty_consumida_real = 0\n",
        "\n",
        "            stock_actual_item_wh = stock_disponible.get(item_code, {}).get(warehouse, 0.0)\n",
        "            if item_code not in productos and stock_disponible.get(item_code):\n",
        "                # Se podr√≠a buscar el nombre en otro lado si se necesita,\n",
        "                # pero por ahora no es crucial para la l√≥gica de stock.\n",
        "                pass\n",
        "\n",
        "            if item_code in ITEMS_CONSUMIR_TODO_STOCK:\n",
        "                # El problema es que esta l√≥gica solo se ejecuta si el √≠tem est√° en `conteo_solicitado`.\n",
        "                # La soluci√≥n es mover el manejo de ITEMS_CONSUMIR_TODO_STOCK a una nueva secci√≥n\n",
        "                # para que se procesen incluso si no fueron expl√≠citamente solicitados.\n",
        "                # Aqu√≠, la l√≥gica original est√° causando que solo se consuman si se solicitan.\n",
        "                # Lo dejar√© como estaba para mostrar que hay que cambiarlo\n",
        "                qty_consumida_real = int(stock_actual_item_wh)\n",
        "                # ... (los print originales) ...\n",
        "            else:\n",
        "                # El resto de los √≠tems (control estricto de solicitado vs. disponible)\n",
        "                qty_consumida_real = min(qty_solicitada, int(stock_actual_item_wh))\n",
        "                # ... (los print originales) ...\n",
        "\n",
        "            consumos_finales[item_code][warehouse]['solicitada'] = qty_solicitada\n",
        "            consumos_finales[item_code][warehouse]['real'] = qty_consumida_real\n",
        "\n",
        "            stock_disponible[item_code][warehouse] -= qty_consumida_real\n",
        "            if stock_disponible[item_code][warehouse] < 0.00001:\n",
        "                stock_disponible[item_code][warehouse] = 0.0\n",
        "\n",
        "    # --- manejar ITEMS_CONSUMIR_TODO_STOCK ---\n",
        "    print(\"\\nüîÑ Procesando √≠tems de 'consumir todo el stock'...\")\n",
        "    for item_code_consumir in ITEMS_CONSUMIR_TODO_STOCK:\n",
        "        # Solo procesar si el √≠tem existe en nuestro diccionario de stock\n",
        "        if item_code_consumir in stock_disponible:\n",
        "            for warehouse, stock_actual in stock_disponible[item_code_consumir].items():\n",
        "                # Verifica que el almac√©n comience con \"51\"\n",
        "                if warehouse.startswith(\"51\"):\n",
        "                    # Si no se solicit√≥ (el valor 'real' es 0), lo actualizamos aqu√≠.\n",
        "                    if consumos_finales[item_code_consumir][warehouse]['real'] == 0 and stock_actual > 0:\n",
        "                        qty_consumida_real = int(stock_actual)\n",
        "\n",
        "                        # Registrar en consumos_finales\n",
        "                        # La cantidad solicitada se deja en 0 ya que no hubo una solicitud expl√≠cita\n",
        "                        consumos_finales[item_code_consumir][warehouse]['solicitada'] = 0\n",
        "                        consumos_finales[item_code_consumir][warehouse]['real'] = qty_consumida_real\n",
        "\n",
        "                        print(f\"‚úÖ Consumiendo {qty_consumida_real} de {item_code_consumir} en {warehouse} (√≠tem de consumo total).\")\n",
        "\n",
        "    # 4. Generar archivo de salida con los consumos finales\n",
        "    filas_totales = 0\n",
        "    with open(archivo_salida, \"w\", newline='', encoding=\"utf-8\") as f_out:\n",
        "        writer = csv.writer(f_out)\n",
        "        writer.writerow([\"item_code\", \"item_name\", \"warehouse\", \"qty_solicitada\", \"qty\"])\n",
        "\n",
        "        for item_code, almacenes_finales in consumos_finales.items():\n",
        "            item_name = productos.get(item_code, f\"‚ùì DESCONOCIDO ({item_code})\")\n",
        "            for warehouse, qtys in almacenes_finales.items():\n",
        "                qty_solicitada_original = qtys['solicitada']\n",
        "                qty_consumida_real = qtys['real']\n",
        "\n",
        "                # Solo escribir si se solicit√≥ o se consumi√≥ algo.\n",
        "                if qty_solicitada_original > 0 or qty_consumida_real > 0:\n",
        "                    writer.writerow([\n",
        "                        item_code,\n",
        "                        item_name,\n",
        "                        warehouse,\n",
        "                        qty_solicitada_original,\n",
        "                        qty_consumida_real\n",
        "                    ])\n",
        "                    filas_totales += 1\n",
        "\n",
        "    print(f\"‚úÖ Archivo '{archivo_salida}' generado con {filas_totales} filas (consumos ajustados por stock y lista ITEMS_CONSUMIR_TODO_STOCK).\")\n",
        "    print(\"‚ú® Proceso de extracci√≥n y c√°lculo de consumibles completado.\")"
      ],
      "metadata": {
        "id": "IqZgELFVvUcp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Procesos previos a la Delivery Note**\n",
        "\n",
        "* `transferirPendientes()`: Esta funci√≥n revisa los equipos marcados para transferencia en los archivos CSV (`columnas_extraidasONT.csv` y `columnas_extraidasDeco.csv`). Si encuentra un equipo que necesita ser transferido de un t√©cnico a otro, se comunica con la API de Frappe para crear un `Stock Entry` (movimiento de stock). Esto asegura que el stock est√© en el almac√©n correcto antes de intentar generar la nota de entrega. Tambi√©n maneja posibles errores de la API e intenta una soluci√≥n alternativa en caso de fallos."
      ],
      "metadata": {
        "id": "GrnpjLJ3vd7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transferirPendientes():\n",
        "    if hay_que_revisar():\n",
        "        return\n",
        "\n",
        "    # Cargar mapa {serie: item_code}\n",
        "    def construir_diccionario_series():\n",
        "        dic = {}\n",
        "        with open(\"series_frappe.csv\", newline='', encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                serie = row[\"name\"].strip()\n",
        "                item_code = row[\"item_code\"].strip()\n",
        "                if serie and item_code:\n",
        "                    dic[serie] = item_code\n",
        "        return dic\n",
        "\n",
        "    HEADERS = {\n",
        "        \"Authorization\": f\"token {API_KEY}:{API_SECRET}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Accept\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    archivos = [\n",
        "        (\"columnas_extraidasONT.csv\", \"ONT (modem fibra)\"),\n",
        "        (\"columnas_extraidasDeco.csv\", \"Deco\")\n",
        "    ]\n",
        "    url_frappe = FRAPPE_API + \"/Stock%20Entry\"\n",
        "\n",
        "    # Construir mapa serie -> item_code\n",
        "    serie_to_item = construir_diccionario_series()\n",
        "\n",
        "    transferencias_por_tecnico = defaultdict(list)\n",
        "\n",
        "    print(\"üöö Procesando transferencias pendientes...\")\n",
        "\n",
        "    # Configurar sesi√≥n con reintentos b√°sicos\n",
        "    session = requests.Session()\n",
        "    adapter = requests.adapters.HTTPAdapter(max_retries=3)\n",
        "    session.mount(\"http://\", adapter)\n",
        "    session.mount(\"https://\", adapter)\n",
        "\n",
        "    for archivo, campo_serie in archivos:\n",
        "        with open(archivo, newline='', encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for fila in reader:\n",
        "                serie = fila.get(campo_serie, \"\").strip()\n",
        "                transferencia = fila.get(\"Transferencia?\", \"\").strip()\n",
        "\n",
        "                if not serie or not transferencia or \",\" not in transferencia:\n",
        "                    continue\n",
        "\n",
        "                origen, destino = [x.strip() for x in transferencia.split(\",\", 1)]\n",
        "                item_code = serie_to_item.get(serie)\n",
        "\n",
        "                if not item_code:\n",
        "                    print(f\"‚ö†Ô∏è No se encontr√≥ item_code para serie '{serie}', omitiendo.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Verificar estado actual de la serie\n",
        "                    url_serie = f\"{FRAPPE_API}/Serial%20No/{serie}\"\n",
        "                    response_serie = session.get(url_serie, headers=HEADERS, timeout=10)\n",
        "\n",
        "                    if response_serie.status_code != 200:\n",
        "                        print(f\"‚ö†Ô∏è No se pudo verificar serie {serie}: {response_serie.status_code}\")\n",
        "                        continue\n",
        "\n",
        "                    serie_data = response_serie.json().get(\"data\", {})\n",
        "                    if serie_data.get(\"warehouse\") != origen:\n",
        "                        print(f\"‚ö†Ô∏è Serie {serie} no est√° en {origen}, est√° en {serie_data.get('warehouse')}\")\n",
        "                        continue\n",
        "\n",
        "                    # Crear la transferencia con enfoque alternativo para bundles\n",
        "                    stock_entry = {\n",
        "                        \"doctype\": \"Stock Entry\",\n",
        "                        \"stock_entry_type\": \"Material Transfer\",\n",
        "                        \"from_warehouse\": origen,\n",
        "                        \"to_warehouse\": destino,\n",
        "                        \"docstatus\": 1,\n",
        "                        \"items\": [{\n",
        "                            \"item_code\": item_code,\n",
        "                            \"qty\": 1,\n",
        "                            \"allow_zero_valuation_rate\": 1,\n",
        "                            \"use_serial_batch_fields\": 1,  # Evitar creaci√≥n de nuevo bundle\n",
        "                            \"serial_no\": serie\n",
        "                        }]\n",
        "                    }\n",
        "\n",
        "                    # Primero intentar con serial_no\n",
        "                    response = session.post(\n",
        "                        url_frappe,\n",
        "                        headers=HEADERS,\n",
        "                        json=stock_entry,\n",
        "                        timeout=30\n",
        "                    )\n",
        "\n",
        "                    if response.status_code == 200:\n",
        "                        docname = response.json().get(\"data\", {}).get(\"name\")  # <- Capturamos el docname\n",
        "                        print(f\"‚úÖ Transferido '{serie}' de '{origen}' a '{destino}'\")\n",
        "                        transferencias_por_tecnico[destino].append(f\"{serie} de {origen}\")\n",
        "                    else:\n",
        "                        error_msg = response.json().get(\"message\", response.text)\n",
        "                        print(f\"‚ùå Error al transferir '{serie}': {response.status_code} - {error_msg}\")\n",
        "\n",
        "                        # Si falla, intentar sin serial_no\n",
        "                        if \"ya est√° creado\" in error_msg:\n",
        "                            print(\"  ‚ö†Ô∏è Intentando soluci√≥n alternativa sin serial_no...\")\n",
        "                            del stock_entry[\"items\"][0][\"serial_no\"]\n",
        "                            response_alt = session.post(\n",
        "                                url_frappe,\n",
        "                                headers=HEADERS,\n",
        "                                json=stock_entry,\n",
        "                                timeout=30\n",
        "                            )\n",
        "                            if response_alt.status_code == 200:\n",
        "                                print(f\"  ‚úÖ Soluci√≥n alternativa exitosa para '{serie}'\")\n",
        "                                transferencias_por_tecnico[destino].append(f\"{serie} de {origen}\")\n",
        "                            else:\n",
        "                                print(f\"  ‚ùå Fall√≥ soluci√≥n alternativa: {response_alt.status_code} - {response_alt.text}\")\n",
        "\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"‚ùå Error de conexi√≥n al transferir '{serie}': {str(e)}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error inesperado al transferir '{serie}': {str(e)}\")\n",
        "                agregar_comentario(\n",
        "                  doctype=\"Stock Entry\",\n",
        "                  docname=docname, # Reemplaza con un docname real de tu Frappe\n",
        "                  contenido=\"Transferencia realizada de forma automatica para una delivery note\"\n",
        "                )\n",
        "    print(\"‚úÖ Transferencias completadas.\")\n",
        "    return transferencias_por_tecnico\n",
        "\n",
        "#transferirPendientes()"
      ],
      "metadata": {
        "id": "3Rjp9aT4vnZF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Santo Grial: Generaci√≥n de la Nota de Entrega**\n",
        "\n",
        "* `deliveryNote(transferencias_por_tecnico)`: Esta es la funci√≥n principal que crea las notas de entrega en Frappe. Agrupa todos los equipos ONT, decodificadores y consumibles por t√©cnico, y luego genera un `Delivery Note` para cada uno. La funci√≥n tambi√©n agrega un comentario en el documento de Frappe que contiene una tabla HTML con los detalles de las √ìrdenes de Trabajo y las transferencias de stock que se realizaron previamente."
      ],
      "metadata": {
        "id": "cMUtmCWrv-WE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "6O_6aTXw7zaV"
      },
      "outputs": [],
      "source": [
        "def deliveryNote(transferencias_por_tecnico):\n",
        "    if hay_que_revisar():\n",
        "        #print(\"üö´ No se gener√≥ la nota de entrega. Revis√° los archivos antes de continuar.\")\n",
        "        return\n",
        "\n",
        "    FRAPPE_API_delivery = f\"{FRAPPE_API}/Delivery%20Note\" # Use FRAPPE_API\n",
        "    HEADERS = {\n",
        "        \"Authorization\": f\"token {API_KEY}:{API_SECRET}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    cliente = \"Instalado\"\n",
        "    archivos_consumir = [archivo_ONT, archivo_DECO]\n",
        "\n",
        "    # Comentarios por t√©cnico desde crudo\n",
        "    def comentarios_en_tabla_html():\n",
        "        comentarios = defaultdict(list)\n",
        "        try:\n",
        "            with open(archivo_crudo, newline='', encoding=\"utf-8\") as f:\n",
        "                reader = csv.reader(f)\n",
        "                # Skip header if present (assuming first row is header in original script)\n",
        "                # next(reader, None)\n",
        "                for fila in reader:\n",
        "                    if len(fila) >= 15:\n",
        "                        almacen = fila[3].strip()\n",
        "                        if not almacen:\n",
        "                            continue\n",
        "                        # Ensure cell content is HTML-safe if necessary (e.g., escape < > &)\n",
        "                        fila_html = \"\".join(f\"<td>{celda.strip()}</td>\" for celda in fila[:15])\n",
        "                        comentarios[almacen].append(f\"<tr>{fila_html}</tr>\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Archivo no encontrado: {archivo_crudo}\")\n",
        "        return comentarios\n",
        "\n",
        "    comentarios_por_almacen = comentarios_en_tabla_html()\n",
        "\n",
        "    # Diccionario de series\n",
        "    dic_series = {}\n",
        "    try:\n",
        "        with open(archivo_Series, newline='', encoding=\"utf-8\") as f_series:\n",
        "            reader = csv.reader(f_series)\n",
        "            next(reader) # Skip header\n",
        "            for fila in reader:\n",
        "                if len(fila) > 5:\n",
        "                    serie = fila[1].strip()\n",
        "                    item_code = fila[3].strip()\n",
        "                    item_name = fila[4].strip()\n",
        "                    dic_series[serie] = {\"item_code\": item_code, \"item_name\": item_name}\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Archivo no encontrado: {archivo_Series}\")\n",
        "\n",
        "    agrupados = defaultdict(lambda: defaultdict(lambda: {\"qty\": 0, \"seriales\": []}))\n",
        "    seriales_usados = set()\n",
        "\n",
        "    for archivo in archivos_consumir:\n",
        "        try:\n",
        "            with open(archivo, newline='', encoding=\"utf-8\") as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                for fila in reader:\n",
        "                    serie = fila.get(\"Consumir\", \"\").strip()\n",
        "                    almacen = fila.get(\"Carg√≥ OT\", \"\").strip()\n",
        "                    if not serie or not almacen:\n",
        "                        continue\n",
        "                    if serie in seriales_usados:\n",
        "                        print(f\"‚ö†Ô∏è Serie duplicada ignorada: {serie}\")\n",
        "                        continue\n",
        "                    if serie not in dic_series:\n",
        "                        print(f\"‚ö†Ô∏è Serie no encontrada en {archivo_Series}: {serie}\")\n",
        "                        continue\n",
        "                    info = dic_series[serie]\n",
        "                    key = (info[\"item_code\"], info[\"item_name\"])\n",
        "                    agrupados[almacen][key][\"qty\"] += 1\n",
        "                    agrupados[almacen][key][\"seriales\"].append(serie)\n",
        "                    seriales_usados.add(serie)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Archivo no encontrado: {archivo}\")\n",
        "\n",
        "    # Consumibles\n",
        "    consumibles_por_almacen = defaultdict(list)\n",
        "    try:\n",
        "        with open(archivo_consumibles, newline='', encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                item_code = row[\"item_code\"].strip()\n",
        "                item_name = row[\"item_name\"].strip()\n",
        "                warehouse = row[\"warehouse\"].strip()\n",
        "                try:\n",
        "                    qty = int(row[\"qty\"])\n",
        "                    if qty > 0:\n",
        "                        consumibles_por_almacen[warehouse].append({\n",
        "                            \"item_code\": item_code,\n",
        "                            \"item_name\": item_name,\n",
        "                            \"qty\": qty,\n",
        "                            \"warehouse\": warehouse\n",
        "                        })\n",
        "                except ValueError:\n",
        "                    print(f\"‚ö†Ô∏è Cantidad inv√°lida en consumibles: {row['qty']} para item {item_name}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Archivo no encontrado: {archivo_consumibles}\")\n",
        "\n",
        "\n",
        "    # Crear delivery note por t√©cnico\n",
        "    todos_los_almacenes = set(agrupados.keys()) | set(consumibles_por_almacen.keys())\n",
        "    for almacen in todos_los_almacenes:\n",
        "        items = []\n",
        "\n",
        "        for (item_code, item_name), datos in agrupados.get(almacen, {}).items():\n",
        "            items.append({\n",
        "                \"item_code\": item_code,\n",
        "                \"item_name\": item_name,\n",
        "                \"qty\": datos[\"qty\"],\n",
        "                \"warehouse\": almacen,\n",
        "                \"serial_no\": \"\\n\".join(datos[\"seriales\"])\n",
        "            })\n",
        "\n",
        "        for prod in consumibles_por_almacen.get(almacen, []):\n",
        "            items.append({\n",
        "                \"item_code\": prod[\"item_code\"],\n",
        "                \"item_name\": prod[\"item_name\"],\n",
        "                \"qty\": prod[\"qty\"],\n",
        "                \"warehouse\": almacen\n",
        "            })\n",
        "\n",
        "        if not items:\n",
        "            print(f\"‚ÑπÔ∏è No hay √≠tems para crear Delivery Note para el almac√©n: {almacen}\")\n",
        "            continue\n",
        "\n",
        "        payload = {\n",
        "            \"customer\": cliente,\n",
        "            \"posting_date\": str(date.today()),\n",
        "            \"set_warehouse\": almacen,\n",
        "            \"docstatus\": 1, # Validar DeliveryNote\n",
        "            \"items\": items,\n",
        "        }\n",
        "\n",
        "        print(f\"üì¶ Enviando nota de entrega para '{almacen}' con {len(items)} √≠tems...\")\n",
        "        response = requests.post(FRAPPE_API_delivery, headers=HEADERS, json=payload)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            docname = response.json()[\"data\"][\"name\"]\n",
        "            print(f\"‚úÖ Nota de entrega creada: {FRAPPE_API_crudo}/app/delivery-note/{docname}\")\n",
        "\n",
        "            # Paso 3: comentario con datos de crudo\n",
        "            filas_html_list = comentarios_por_almacen.get(almacen, [])\n",
        "            if filas_html_list:\n",
        "                full_comment_html = \"<h3>Detalle del Consumo (Crudo):</h3>\"\n",
        "                full_comment_html += \"<table border='1' style='width:100%; border-collapse: collapse;'>\"\n",
        "                full_comment_html += \"<thead><tr>\" + \"\".join([f\"<th>Col{i+1}</th>\" for i in range(15)]) + \"</tr></thead>\" # Add a simple header for clarity\n",
        "                full_comment_html += \"<tbody>\"\n",
        "                full_comment_html += \"\".join(filas_html_list) # Join the list of <tr>...</tr> strings\n",
        "                full_comment_html += \"</tbody></table>\"\n",
        "\n",
        "                agregar_comentario(\n",
        "                  doctype=\"Delivery Note\",\n",
        "                  docname=docname,\n",
        "                  contenido=full_comment_html\n",
        "                )\n",
        "\n",
        "            # Paso 4: comentario con transferencias realizadas a este t√©cnico\n",
        "            transferencias_list = transferencias_por_tecnico.get(almacen)\n",
        "            if transferencias_list:\n",
        "                transfer_comment_html = \"<h3>Transferencias Relacionadas:</h3>\"\n",
        "                transfer_comment_html += \"<table border='1' style='width:100%; border-collapse: collapse;'>\"\n",
        "                transfer_comment_html += \"<tbody>\"\n",
        "                transfer_comment_html += \"üöö \".join([f\"<tr><td>{t}</td></tr>\" for t in transferencias_list]) # Join the list of <tr>...</tr> strings\n",
        "                transfer_comment_html += \"</tbody></table>\"\n",
        "\n",
        "                agregar_comentario(\n",
        "                  doctype=\"Delivery Note\",\n",
        "                  docname=docname,\n",
        "                  contenido=transfer_comment_html\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            print(f\"‚ùå Error al crear nota de entrega para {almacen}:\")\n",
        "            print(response.status_code, response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OCR"
      ],
      "metadata": {
        "id": "GqOiY8nOwQFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "19crc6NgwR_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Descarga de im√°genes**\n",
        "\n",
        "* `extraer_file_id_google_drive(url)`: Esta funci√≥n extrae el ID √∫nico de un archivo de Google Drive a partir de su URL. Es una funci√≥n de utilidad que permite al script identificar la imagen que necesita descargar.\n",
        "* `download_image_from_drive(file_id, dest_path)`: Utiliza el ID del archivo de Google Drive para descargar la imagen y la guarda en una ruta local. Tambi√©n realiza una verificaci√≥n b√°sica para asegurarse de que el archivo descargado sea realmente una imagen v√°lida.\n",
        "* `extraer_links_unicos_de_csvs(csvs, columna=\"foto (no recibido)\")`: Recorre los archivos CSV de ONT y Decos para encontrar todas las URLs de las fotos de los equipos que no fueron recibidos. Devuelve una lista de URLs √∫nicas y un diccionario que mapea cada URL con el n√∫mero de serie correspondiente, para saber qu√© imagen pertenece a qu√© equipo."
      ],
      "metadata": {
        "id": "4POZ5l_WzMZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_file_id_google_drive(url):\n",
        "    for patron in [r'id=([a-zA-Z0-9_\\-]+)', r'/d/([a-zA-Z0-9_\\-]+)']:\n",
        "        m = re.search(patron, url)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    return None\n",
        "\n",
        "def download_image_from_drive(file_id, dest_path):\n",
        "    url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "    r = requests.get(url)\n",
        "    if r.status_code == 200:\n",
        "        with open(dest_path, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "\n",
        "        # Validar si es imagen v√°lida\n",
        "        try:\n",
        "            img = cv2.imread(dest_path)\n",
        "            if img is None:\n",
        "                print(f\"‚ö†Ô∏è Archivo no es una imagen v√°lida: {file_id}\")\n",
        "                os.remove(dest_path)\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error leyendo imagen: {file_id} - {e}\")\n",
        "            os.remove(dest_path)\n",
        "            return False\n",
        "\n",
        "        print(f\"üì• Imagen descargada en {dest_path}\")\n",
        "        return True\n",
        "\n",
        "    print(f\"‚ùå Error al descargar {file_id} (HTTP {r.status_code})\")\n",
        "    return False\n",
        "\n",
        "\n",
        "def extraer_links_unicos_de_csvs(csvs, columna=\"foto (no recibido)\"):\n",
        "    enlaces_unicos = set()\n",
        "    mapeo_url_a_serie = {}\n",
        "\n",
        "    for path in csvs:\n",
        "        with open(path, encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            headers = reader.fieldnames\n",
        "\n",
        "            # Detectar columna de serie (puede variar entre ONT o Deco)\n",
        "            posibles_series = [\"serie\", \"ONT (modem fibra)\", \"Deco\", \"Consumir\", \"No recibido\"]\n",
        "            col_serie = next((c for c in posibles_series if c in headers), None)\n",
        "\n",
        "            if not col_serie:\n",
        "                continue\n",
        "\n",
        "            for i, fila in enumerate(reader):\n",
        "                origen = fila.get(\"Origen equipo\", \"\").strip().lower()\n",
        "                if origen != \"no\":\n",
        "                    continue\n",
        "\n",
        "                serie = fila.get(col_serie, \"\").strip().upper()\n",
        "                raw_links = fila.get(columna, \"\")\n",
        "\n",
        "                if not serie:\n",
        "                    continue\n",
        "                if not raw_links:\n",
        "                    continue\n",
        "\n",
        "                for url in raw_links.splitlines():\n",
        "                    url = url.strip()\n",
        "                    if url.startswith(\"http\"):\n",
        "                        enlaces_unicos.add(url)\n",
        "                        mapeo_url_a_serie[url] = serie\n",
        "\n",
        "    return list(enlaces_unicos), mapeo_url_a_serie"
      ],
      "metadata": {
        "id": "M-g_IjeEzLfE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Rotaci√≥n de las imagenes**\n",
        "\n",
        "* `rotar_imagen_cv2(img, angle)`: Esta es una funci√≥n de utilidad que rota una imagen en un √°ngulo espec√≠fico (90, 180 o 270 grados) usando la biblioteca `cv2`.\n",
        "* `asegurar_apaisado(img)`: Rota autom√°ticamente una imagen a una orientaci√≥n horizontal si es que esta se encuentra en una orientaci√≥n vertical.\n",
        "* `ocr_en_varias_rotaciones(img_path, ocr_detector, prefijos_validos)`: Esta es la funci√≥n principal de OCR. Realiza un reconocimiento de texto en varias rotaciones de una imagen (0 y 180 grados) para asegurar una lectura correcta. Da prioridad a la rotaci√≥n que encuentre un n√∫mero de serie con uno de los `prefijos_validos`. Si no encuentra ning√∫n prefijo v√°lido, devuelve el resultado de la rotaci√≥n que haya detectado m√°s texto como un intento de respaldo."
      ],
      "metadata": {
        "id": "C1YpjxYezUup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotar_imagen_cv2(img, angle):\n",
        "    if angle == 90 or angle == 270:\n",
        "        return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE if angle == 90 else cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "    elif angle == 180:\n",
        "        return cv2.rotate(img, cv2.ROTATE_180)\n",
        "    return img\n",
        "\n",
        "def asegurar_apaisado(img):\n",
        "    h, w = img.shape[:2]\n",
        "    return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE) if h > w else img\n",
        "\n",
        "\n",
        "def ocr_en_varias_rotaciones(img_path, ocr_detector, prefijos_validos):\n",
        "    \"\"\"\n",
        "    Realiza OCR en varias rotaciones de una imagen, priorizando la rotaci√≥n\n",
        "    que contenga un texto con un prefijo v√°lido.\n",
        "    \"\"\"\n",
        "    img = asegurar_apaisado(cv2.imread(img_path))\n",
        "    mejor_resultado = None\n",
        "    mejor_resultado_con_prefijo = None\n",
        "    max_textos = 0\n",
        "\n",
        "    for angle in [0, 180]:\n",
        "        print(f\"\\nüîÑ Rotaci√≥n {angle}¬∞\")\n",
        "        rotada = rotar_imagen_cv2(img, angle)\n",
        "        temp = \"/tmp/temp_ocr.jpg\"\n",
        "        cv2.imwrite(temp, rotada)\n",
        "        resultado = ocr_detector.predict(temp)\n",
        "        os.remove(temp)\n",
        "\n",
        "        if not resultado:\n",
        "            continue\n",
        "\n",
        "        textos = resultado[0].get('rec_texts', [])\n",
        "        print(\"üìå Textos:\", textos)\n",
        "\n",
        "        # --- Nueva l√≥gica de selecci√≥n ---\n",
        "        contiene_prefijo = False\n",
        "        texto_con_prefijo = None\n",
        "        for t in textos:\n",
        "            texto_limpio = t.strip().upper()\n",
        "            if any(texto_limpio.startswith(p) for p in prefijos_validos):\n",
        "                contiene_prefijo = True\n",
        "                texto_con_prefijo = t  # Guarda el texto completo que contiene el prefijo\n",
        "                break\n",
        "\n",
        "        # Si se encuentra un prefijo v√°lido en esta rotaci√≥n, la guardamos como la mejor.\n",
        "        if contiene_prefijo:\n",
        "            mejor_resultado_con_prefijo = resultado\n",
        "            # No necesitamos seguir buscando, esta es la mejor opci√≥n.\n",
        "            break\n",
        "\n",
        "        # Si no hay prefijo v√°lido, nos quedamos con la rotaci√≥n que tenga m√°s texto\n",
        "        # para tener un respaldo en caso de que no haya coincidencias de prefijos en ninguna rotaci√≥n.\n",
        "        if len(textos) > max_textos:\n",
        "            mejor_resultado = resultado\n",
        "            max_textos = len(textos)\n",
        "\n",
        "    # Si encontramos un resultado con un prefijo v√°lido, lo retornamos\n",
        "    if mejor_resultado_con_prefijo:\n",
        "        print(\"‚úÖ Se encontr√≥ una rotaci√≥n con prefijo v√°lido. Retornando el resultado.\")\n",
        "        return mejor_resultado_con_prefijo\n",
        "\n",
        "    # Si no, retornamos el resultado que tenga la mayor cantidad de texto\n",
        "    if mejor_resultado:\n",
        "        print(\"‚ÑπÔ∏è No se encontraron prefijos v√°lidos. Retornando el resultado con m√°s texto.\")\n",
        "    else:\n",
        "        print(\"‚ùå No se detect√≥ ning√∫n texto en ninguna rotaci√≥n.\")\n",
        "\n",
        "    return mejor_resultado"
      ],
      "metadata": {
        "id": "XkLPbnQdzZEy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Filtrado y comparaci√≥n de series**\n",
        "\n",
        "* `filtrar_numeros_serie(resultados)`: Esta funci√≥n toma los resultados del OCR y los filtra para encontrar n√∫meros de serie v√°lidos. Busca cadenas de texto que comiencen con cualquiera de los `prefijos_validos` y se asegura de que la cadena resultante tenga al menos 6 caracteres y sea alfanum√©rica.\n",
        "* `cargar_series_no_recibidas(csvs)`: Lee los archivos CSV y extrae una lista de tuplas, donde cada tupla contiene el n√∫mero de serie original y la URL de la foto del equipo que no fue recibido. Esto prepara los datos para el proceso de correcci√≥n.\n",
        "* `procesar_fotos_con_ocr(...)`: Esta es la funci√≥n principal que coordina el proceso de OCR. Primero, consulta la API de Frappe para obtener la lista completa de n√∫meros de serie v√°lidos. Luego, descarga las im√°genes de los equipos no recibidos y ejecuta el OCR en cada una. Finalmente, compara las series detectadas por el OCR con la base de datos de Frappe para validar las correcciones y las exporta a un archivo CSV.\n",
        "* `run_ocr_si_hay_que_revisar(...)`: Act√∫a como la funci√≥n de entrada para el proceso de OCR. Llama a `extraer_links_unicos_de_csvs` para obtener las URLs de las fotos de los equipos no recibidos y luego a `procesar_fotos_con_ocr` para ejecutar el reconocimiento.\n",
        "* `exportar_series_detectadas_csv(...)`: Genera un archivo CSV que mapea cada serie original (la que no fue recibida) con la serie corregida que fue detectada y validada por el OCR, facilitando la correcci√≥n manual o autom√°tica.\n",
        "* `obtener_mapeo_file_id_a_idx(no_recibidos)` y `exportar_url_serie_a_csv(...)`: Funciones de utilidad para manejar la relaci√≥n entre los IDs de archivo, las URLs y los n√∫meros de serie originales.\n",
        "* `comparar_series_viejos_y_nuevos(...)`: Compara las series originales con las series corregidas del OCR para identificar posibles errores. Utiliza el algoritmo de distancia de Levenshtein para sugerir las correcciones m√°s probables, lo que ayuda a los usuarios a validar los resultados del OCR."
      ],
      "metadata": {
        "id": "uYVVpS4ezuXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "kkOCc4LKAqlJ"
      },
      "outputs": [],
      "source": [
        "def filtrar_numeros_serie(resultados):\n",
        "    \"\"\"\n",
        "    Filtra los n√∫meros de serie de los resultados del OCR.\n",
        "    La l√≥gica se basa en buscar prefijos v√°lidos dentro de la cadena de texto\n",
        "    y extraer la parte del texto que comienza con ese prefijo.\n",
        "\n",
        "    Args:\n",
        "        resultados (list): Una lista de resultados de OCR, donde cada elemento es un diccionario\n",
        "                         con una clave 'rec_texts' que contiene una lista de strings.\n",
        "\n",
        "    Returns:\n",
        "        list: Una lista de strings √∫nicos que son n√∫meros de serie v√°lidos.\n",
        "    \"\"\"\n",
        "    encontrados = set()\n",
        "\n",
        "    for bloque in resultados:\n",
        "        for txt in bloque.get('rec_texts', []):\n",
        "            t = txt.strip().upper()\n",
        "\n",
        "            for prefijo in prefijos_validos:\n",
        "                if prefijo in t:\n",
        "                    inicio = t.find(prefijo)\n",
        "\n",
        "                    numero_serie_candidato = t[inicio:]\n",
        "\n",
        "                    if len(numero_serie_candidato) >= 6 and numero_serie_candidato.isalnum():\n",
        "                        encontrados.add(numero_serie_candidato)\n",
        "                        break\n",
        "\n",
        "    return list(encontrados)\n",
        "\n",
        "\n",
        "def cargar_series_no_recibidas(csvs):\n",
        "    \"\"\"\n",
        "    Retorna una lista de tuplas: (serie_original, link_individual)\n",
        "    Si hay m√°s de un link en la celda, los divide correctamente.\n",
        "    \"\"\"\n",
        "    pares = []\n",
        "    for path in csvs:\n",
        "        with open(path, encoding=\"utf-8\") as f:\n",
        "            for fila in csv.DictReader(f):\n",
        "                serie = fila.get(\"serie\", \"\").strip().upper()\n",
        "                enlaces = fila.get(\"foto (no recibido)\", \"\").strip()\n",
        "                if not serie or not enlaces:\n",
        "                    continue\n",
        "                for link in enlaces.splitlines():\n",
        "                    if link.strip().startswith(\"http\"):\n",
        "                        pares.append((serie, link.strip()))\n",
        "    return pares\n",
        "\n",
        "\n",
        "def procesar_fotos_con_ocr(enlaces, ocr_detector, api_key, api_secret, frappe_api, no_recibidos=None):\n",
        "    print(\"\\nüîå Obteniendo series del ERP...\")\n",
        "    try:\n",
        "        headers = {\"Authorization\": f\"token {api_key}:{api_secret}\"}\n",
        "        url = f\"{frappe_api}/Serial%20No?fields=[\\\"name\\\"]&limit_page_length=1000\"\n",
        "        data = requests.get(url, headers=headers).json().get(\"data\", [])\n",
        "        series_erp = set(x[\"name\"].upper() for x in data if x.get(\"name\"))\n",
        "        print(f\"‚úÖ {len(series_erp)} series obtenidas\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error ERP: {e}\")\n",
        "        return {}\n",
        "\n",
        "    resultados, mapeos = {}, []\n",
        "    filas_validas = []\n",
        "\n",
        "    for i, url in enumerate(enlaces):\n",
        "        print(f\"\\nüì∑ Imagen {i+1}/{len(enlaces)}: {url}\")\n",
        "        file_id = extraer_file_id_google_drive(url)\n",
        "        if not file_id:\n",
        "            print(\"‚ö†Ô∏è No se pudo extraer ID\")\n",
        "            continue\n",
        "\n",
        "        path = f\"/tmp/imagen_{i}.jpg\"\n",
        "\n",
        "        if not download_image_from_drive(file_id, path):\n",
        "            continue\n",
        "\n",
        "        ocr = ocr_en_varias_rotaciones(path, ocr_detector, prefijos_validos)\n",
        "        series_detectadas = filtrar_numeros_serie(ocr or [])\n",
        "        verificacion = {s: s in series_erp for s in series_detectadas}\n",
        "\n",
        "        print(\"üîé Series detectadas:\", series_detectadas)\n",
        "        print(\"‚úÖ Validaci√≥n:\", verificacion)\n",
        "\n",
        "        for s in series_detectadas:\n",
        "            if verificacion.get(s):\n",
        "                filas_validas.append((url, s))\n",
        "\n",
        "        if no_recibidos:\n",
        "            for detectada in series_detectadas:\n",
        "                for original, link in no_recibidos.items():\n",
        "                    if file_id in link and detectada != original:\n",
        "                        mapeos.append(f\"{original} -> {detectada}\")\n",
        "\n",
        "        resultados[url] = {\n",
        "            \"series_detectadas\": series_detectadas,\n",
        "            \"verificacion\": verificacion\n",
        "        }\n",
        "\n",
        "        os.remove(path)\n",
        "\n",
        "    if filas_validas:\n",
        "        with open(path_nuevos, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"url\", \"serie\"])\n",
        "            writer.writerows(filas_validas)\n",
        "        print(f\"\\n‚úÖ CSV generado: series_validas_detectadas.csv ({len(filas_validas)} filas)\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è No se detectaron series v√°lidas, no se gener√≥ CSV.\")\n",
        "\n",
        "    return {\n",
        "        \"resultados\": resultados,\n",
        "        \"mapeos_sugeridos\": mapeos\n",
        "    }\n",
        "\n",
        "\n",
        "def run_ocr_si_hay_que_revisar(\n",
        "    ocr_detector,\n",
        "    api_key,\n",
        "    api_secret,\n",
        "    frappe_api,\n",
        "    archivo_ONT,\n",
        "    archivo_DECO\n",
        "):\n",
        "    print(\"‚ö†Ô∏è Ejecutando OCR por no recibidos...\")\n",
        "    enlaces = extraer_links_unicos_de_csvs([archivo_ONT, archivo_DECO])[0]\n",
        "    print(f\"üîó {len(enlaces)} enlaces √∫nicos\")\n",
        "\n",
        "    no_recibidos = cargar_series_no_recibidas([archivo_ONT, archivo_DECO])\n",
        "    resultado = procesar_fotos_con_ocr(enlaces, ocr_detector, api_key, api_secret, frappe_api, no_recibidos)\n",
        "\n",
        "    for mapeo in resultado[\"mapeos_sugeridos\"]:\n",
        "        print(\"üîÅ\", mapeo)\n",
        "\n",
        "    return resultado\n",
        "\n",
        "def exportar_series_detectadas_csv(\n",
        "    resultados_ocr: dict,\n",
        "    url_a_serie_original: dict,\n",
        "    path_salida=\"series_detectadas_con_validacion.csv\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Exporta un CSV con:\n",
        "    - serie original (de los no recibidos)\n",
        "    - serie corregida detectada y validada por OCR\n",
        "\n",
        "    Usa la URL como clave para relacionar datos.\n",
        "    \"\"\"\n",
        "\n",
        "    filas = []\n",
        "\n",
        "    for url, serie_original in url_a_serie_original.items():\n",
        "        resultado = resultados_ocr.get(url)\n",
        "        if not resultado:\n",
        "            continue\n",
        "        if \"verificacion\" not in resultado:\n",
        "            continue\n",
        "\n",
        "        # Filtrar solo series validadas True\n",
        "        series_validas = [s for s, ok in resultado[\"verificacion\"].items() if ok]\n",
        "\n",
        "        # Si hay series v√°lidas, hacer una fila por cada una\n",
        "        for serie_corregida in series_validas:\n",
        "            filas.append([serie_original, serie_corregida])\n",
        "\n",
        "    # Guardar CSV con SOLO dos columnas: serie original y serie corregida\n",
        "    with open(path_salida, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['serie_original', 'serie_corregida_valida'])\n",
        "        writer.writerows(filas)\n",
        "\n",
        "    print(f\"‚úÖ CSV generado: {path_salida} ({len(filas)} filas)\")\n",
        "\n",
        "\n",
        "\n",
        "def obtener_mapeo_file_id_a_idx(no_recibidos):\n",
        "    \"\"\"\n",
        "    Devuelve un dict: file_id => √≠ndice en la lista de no_recibidos\n",
        "    \"\"\"\n",
        "    mapeo = {}\n",
        "    for idx, (_, url) in enumerate(no_recibidos):\n",
        "        file_id = extraer_file_id_google_drive(url)\n",
        "        if file_id:\n",
        "            mapeo[file_id] = idx\n",
        "    return mapeo\n",
        "\n",
        "def exportar_url_serie_a_csv(url_a_serie_dict, path_salida=path_viejos):\n",
        "    \"\"\"\n",
        "    Exporta un CSV con dos columnas:\n",
        "    url, serie\n",
        "    a partir de un diccionario {url: serie}\n",
        "    \"\"\"\n",
        "    filas = [(url, serie) for url, serie in url_a_serie_dict.items()]\n",
        "\n",
        "    with open(path_salida, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['url', 'serie'])\n",
        "        writer.writerows(filas)\n",
        "\n",
        "    print(f\"‚úÖ CSV generado: {path_salida} ({len(filas)} filas)\")\n",
        "\n",
        "\n",
        "def comparar_series_viejos_y_nuevos(\n",
        "    path_viejos=path_viejos,\n",
        "    path_nuevos=path_nuevos\n",
        "):\n",
        "    # Verificar existencia de archivos\n",
        "    if not os.path.exists(path_viejos):\n",
        "        print(f\"‚ùå Archivo no encontrado: {path_viejos}\")\n",
        "        return\n",
        "    elif not os.path.exists(path_nuevos):\n",
        "        print(f\"‚ùå Archivo no encontrado: {path_nuevos}\")\n",
        "        return\n",
        "\n",
        "    def leer_csv(path):\n",
        "        d = {}\n",
        "        with open(path, encoding=\"utf-8\") as f:\n",
        "            for row in csv.DictReader(f):\n",
        "                url = row[\"url\"].strip()\n",
        "                serie = row[\"serie\"].strip().upper()\n",
        "                d.setdefault(url, []).append(serie)\n",
        "        return d\n",
        "\n",
        "    def levenshtein(a, b):\n",
        "        if len(a) < len(b):\n",
        "            return levenshtein(b, a)\n",
        "        if len(b) == 0:\n",
        "            return len(a)\n",
        "        previous_row = range(len(b) + 1)\n",
        "        for i, c1 in enumerate(a):\n",
        "            current_row = [i + 1]\n",
        "            for j, c2 in enumerate(b):\n",
        "                insertions = previous_row[j + 1] + 1\n",
        "                deletions = current_row[j] + 1\n",
        "                substitutions = previous_row[j] + (c1 != c2)\n",
        "                current_row.append(min(insertions, deletions, substitutions))\n",
        "            previous_row = current_row\n",
        "        return previous_row[-1]\n",
        "\n",
        "    def sugerir_correccion(serie_vieja, series_validas):\n",
        "        if not series_validas:\n",
        "            return \"\"\n",
        "        similitudes = [(s, levenshtein(serie_vieja, s)) for s in series_validas]\n",
        "        similitudes.sort(key=lambda x: x[1])\n",
        "        return similitudes[0][0]  # menor distancia\n",
        "\n",
        "    viejos = leer_csv(path_viejos)\n",
        "    nuevos = leer_csv(path_nuevos)\n",
        "    todas_las_urls = sorted(set(viejos) | set(nuevos))\n",
        "\n",
        "    print(\"\\nüìä Comparaci√≥n de series:\")\n",
        "    for url in todas_las_urls:\n",
        "        viejas = viejos.get(url, [])\n",
        "        nuevas = nuevos.get(url, [])\n",
        "        print(f\"{url} - {viejas} -> {nuevas}\")\n",
        "\n",
        "    print(\"\\nüîç Sugerencias autom√°ticas de correcci√≥n:\")\n",
        "    for url in todas_las_urls:\n",
        "        viejas = viejos.get(url, [])\n",
        "        nuevas = nuevos.get(url, [])\n",
        "        for vieja in viejas:\n",
        "            sugerida = sugerir_correccion(vieja, nuevas)\n",
        "            if sugerida and sugerida != vieja:\n",
        "                print(f\"üîÅ {url} - Corregir '{vieja}' -> '{sugerida}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Funci√≥n principal (`main`)**\n",
        "\n",
        "`main()`: Esta es la funci√≥n principal que orquesta todo el flujo de trabajo del script. Ejecuta las siguientes etapas de forma secuencial:\n",
        "1.  **Limpieza inicial**: Elimina archivos temporales de ejecuciones anteriores.\n",
        "2.  **Obtenci√≥n de datos**: Llama a las funciones para descargar la planilla de consumo y los datos de stock y series de Frappe.\n",
        "3.  **Filtrado y procesamiento**: Procesa los datos para extraer la informaci√≥n de equipos ONT, decodificadores y consumibles.\n",
        "4.  **Transferencias**: Revisa si hay equipos que necesitan ser transferidos entre t√©cnicos y realiza las transferencias necesarias en el ERP.\n",
        "5.  **Generaci√≥n de notas de entrega**: Crea las notas de entrega (`Delivery Notes`) en Frappe, agrupando los √≠tems por t√©cnico y agregando comentarios detallados.\n",
        "6.  **OCR condicional**: Si a√∫n quedan equipos marcados como \"no recibidos\", inicia un proceso de OCR (Reconocimiento √ìptico de Caracteres). Este subproceso instala las librer√≠as necesarias, descarga las fotos, las procesa para leer los n√∫meros de serie, los valida con el ERP y genera un reporte de correcciones sugeridas.\n",
        "7.  **Limpieza final**: Despu√©s de completar el flujo principal o el OCR, limpia los archivos CSV temporales."
      ],
      "metadata": {
        "id": "MsNl0kzJ3CZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    #os.system(\"clear\") #quitar comentario en caso de usar en local, para mas estetica\n",
        "\n",
        "    # Limpieza previa\n",
        "    borrar_archivos_csv() #quitar comentario en caso de usar en local\n",
        "\n",
        "    # Obtenci√≥n de datos\n",
        "    obtenerCrudo()\n",
        "    obtenerSeriesFrappe()\n",
        "    obtenerProductos()\n",
        "\n",
        "    # Procesamiento\n",
        "    extraer_columnas_ont()\n",
        "    extraer_columnas_deco()\n",
        "    extraer_consumibles()\n",
        "\n",
        "    # Transferencias\n",
        "    transferencias_por_tecnico = transferirPendientes()\n",
        "\n",
        "    # Si tiene seguro que abre, sino no\n",
        "    #abrirFotosNoRecibidos() # en el caso de colab no hace falta jsajdsja\n",
        "\n",
        "    # Deliverys\n",
        "    print(\"\\n\\n--- Generando Delivery Note ---\")\n",
        "    deliveryNote(transferencias_por_tecnico)\n",
        "\n",
        "    # Ahora chequeo si hay que revisar con OCR:\n",
        "    if hay_que_revisar(only_boolean=True):\n",
        "        print(\"\\n--- Revisando dependencias para comenzar con el OCR ---\")\n",
        "\n",
        "        \"\"\"OCR batch desde enlaces de Google Drive con rotaci√≥n autom√°tica\"\"\"\n",
        "        # Instalaciones de dependencias de ocr(solo en Colab)\n",
        "        # solo si hace falta\n",
        "        !pip install pytesseract\n",
        "        !apt-get install -y tesseract-ocr libtesseract-dev\n",
        "        if gpu_o_cpu==\"C\" or gpu_o_cpu == \"\":\n",
        "          !pip install paddlepaddle -f https://www.paddlepaddle.org.cn/whl/quick_install.html # en caso de usar local para poder usar el procesamiento a trav√©s de CPU y no de grafica, tarda mas solamente\n",
        "        if gpu_o_cpu.upper() == \"G\":\n",
        "          !pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n",
        "        !pip install paddleocr\n",
        "\n",
        "        from paddleocr import PaddleOCR\n",
        "\n",
        "        print(\"\\n--- Lanzando OCR por equipos no recibidos ---\")\n",
        "\n",
        "        ocr_detector = PaddleOCR(det_model_dir=None, use_textline_orientation=True, lang='en')\n",
        "\n",
        "        resultado_ocr = run_ocr_si_hay_que_revisar(\n",
        "            ocr_detector,\n",
        "            API_KEY,\n",
        "            API_SECRET,\n",
        "            FRAPPE_API,\n",
        "            archivo_ONT,\n",
        "            archivo_DECO\n",
        "        )\n",
        "\n",
        "        # Cargar no_recibidos como lista de tuplas (serie_original, url)\n",
        "        no_recibidos = cargar_series_no_recibidas([archivo_ONT, archivo_DECO])\n",
        "\n",
        "        # Convertir a dict {url: serie_original}\n",
        "        url_a_serie_original = {url: serie for serie, url in no_recibidos}\n",
        "\n",
        "        # series mal\n",
        "        url_a_serie = extraer_links_unicos_de_csvs([archivo_ONT, archivo_DECO])[1]\n",
        "        exportar_url_serie_a_csv(url_a_serie)\n",
        "\n",
        "        comparar_series_viejos_y_nuevos()\n",
        "        borrar_archivos_csv()\n",
        "\n",
        "    else:\n",
        "        print(\"\\n--- Todos los equipos recibidos, seguimos con flujo normal ---\")\n",
        "        ultimaOTconsumida()\n",
        "        borrar_archivos_csv()  # sacar el comentario en caso de usar local para que se puedan actualizar los archivos descargados\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "PDVRh5b9BEMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b0409c-64ce-44cc-be00-0ab39b18881b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Archivo eliminado: crudo.csv\n",
            "üßπ Archivo eliminado: series_frappe.csv\n",
            "üßπ Archivo eliminado: productos.csv\n",
            "üßπ Archivo eliminado: columnas_extraidasONT.csv\n",
            "üßπ Archivo eliminado: columnas_extraidasDeco.csv\n",
            "üßπ Archivo eliminado: reporte_consumibles.csv\n",
            "Descargando CSV...\n",
            "Filtrando filas que no hayan sido consumidas...\n",
            "‚úÖ Filtrado completo. 30 filas guardadas en 'crudo.csv'\n",
            "Consultando series en Frappe...\n",
            "‚úÖ 1000 series guardadas en 'series_frappe.csv'\n",
            "Iniciando descarga de stock proyectado...\n",
            "Se obtuvieron 104 registros de stock\n",
            "‚úÖ Datos guardados correctamente en productos.csv\n",
            "‚úÖ Archivo 'columnas_extraidasONT.csv' generado con 21 filas filtradas.\n",
            "‚úÖ Archivo 'columnas_extraidasDeco.csv' generado con 19 filas.\n",
            "üõ†Ô∏è Iniciando extracci√≥n y c√°lculo de consumibles...\n",
            "‚ö†Ô∏è Valor no num√©rico en columna O de 'crudo.csv' para 51ROD636 (Pat. AD833WB) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "‚ö†Ô∏è Valor no num√©rico en columna O de 'crudo.csv' para 51ROD637 (Pat. AC774WF) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "‚ö†Ô∏è Valor no num√©rico en columna O de 'crudo.csv' para 51ROD637 (Pat. AC774WF) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "‚ö†Ô∏è Valor no num√©rico en columna O de 'crudo.csv' para 51ROD637 (Pat. AC774WF) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "‚ö†Ô∏è Valor no num√©rico en columna O de 'crudo.csv' para 51ROD636 (Pat. AD833WB) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "‚ö†Ô∏è Valor no num√©rico en columna O de 'crudo.csv' para 51ROD636 (Pat. AD833WB) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "‚ö†Ô∏è Valor no num√©rico en columna O de 'crudo.csv' para 51ROD465 (Pat. AE835PS) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "‚ö†Ô∏è Valor no num√©rico en columna O de 'crudo.csv' para 51ROD465 (Pat. AE835PS) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "\n",
            "üîÑ Procesando √≠tems de 'consumir todo el stock'...\n",
            "‚úÖ Consumiendo 100 de 170300004 en 51ROD465 (Pat. AE835PS) - QPS (√≠tem de consumo total).\n",
            "‚úÖ Archivo 'reporte_consumibles.csv' generado con 21 filas (consumos ajustados por stock y lista ITEMS_CONSUMIR_TODO_STOCK).\n",
            "‚ú® Proceso de extracci√≥n y c√°lculo de consumibles completado.\n",
            "üöö Procesando transferencias pendientes...\n",
            "‚úÖ Transferido 'FZ324060208468' de '51ROD465 (Pat. AE835PS) - QPS' a '51ROD636 (Pat. AD833WB) - QPS'\n",
            "üó®Ô∏è Comentario agregado a MAT-STE-2025-01380 (Stock Entry) usando add_comment\n",
            "‚úÖ Transferencias completadas.\n",
            "\n",
            "\n",
            "--- Generando Delivery Note ---\n",
            "üì¶ Enviando nota de entrega para '51ROD637 (Pat. AC774WF) - QPS' con 8 √≠tems...\n",
            "‚úÖ Nota de entrega creada: https://ad3b02a9cc80.ngrok-free.app/app/delivery-note/MAT-DN-2025-00143\n",
            "üó®Ô∏è Comentario agregado a MAT-DN-2025-00143 (Delivery Note) usando add_comment\n",
            "üì¶ Enviando nota de entrega para '51ROD636 (Pat. AD833WB) - QPS' con 8 √≠tems...\n",
            "‚úÖ Nota de entrega creada: https://ad3b02a9cc80.ngrok-free.app/app/delivery-note/MAT-DN-2025-00144\n",
            "üó®Ô∏è Comentario agregado a MAT-DN-2025-00144 (Delivery Note) usando add_comment\n",
            "üó®Ô∏è Comentario agregado a MAT-DN-2025-00144 (Delivery Note) usando add_comment\n",
            "üì¶ Enviando nota de entrega para '51ROD465 (Pat. AE835PS) - QPS' con 9 √≠tems...\n",
            "‚úÖ Nota de entrega creada: https://ad3b02a9cc80.ngrok-free.app/app/delivery-note/MAT-DN-2025-00145\n",
            "üó®Ô∏è Comentario agregado a MAT-DN-2025-00145 (Delivery Note) usando add_comment\n",
            "\n",
            "--- Todos los equipos recibidos, seguimos con flujo normal ---\n",
            "\n",
            "\n",
            "√öltima OT del crudo.csv: SA489369\n",
            "Enviado a Forms\n",
            "\n",
            "\n",
            "üßπ Archivo eliminado: crudo.csv\n",
            "üßπ Archivo eliminado: series_frappe.csv\n",
            "üßπ Archivo eliminado: productos.csv\n",
            "üßπ Archivo eliminado: columnas_extraidasONT.csv\n",
            "üßπ Archivo eliminado: columnas_extraidasDeco.csv\n",
            "üßπ Archivo eliminado: reporte_consumibles.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQYBzdC52Db4+wJP9xgybz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}