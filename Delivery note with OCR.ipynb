{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OffShur3/NixosGenerations/blob/main/Delivery%20note%20with%20OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nota de entrega con OCR\n",
        "En este programa nos genera notas de entrega para todos los consumos pendientes de las instalaciones, tiene un modo de reconocimiento de texto dentro de las fotos para poder detectar en caso de que haya algun serie mal escrito"
      ],
      "metadata": {
        "id": "vQgz0ijO-kW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importacion de librerias\n",
        "Esto es lo necesario para que el programa funcione, sin contar con el reconocimiento de OCR que aparece luego en caso de que se necesite"
      ],
      "metadata": {
        "id": "yFGYWBJJQS32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importacion de librerias\n",
        "import csv\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import pprint\n",
        "import cv2\n",
        "from time import sleep\n",
        "from datetime import date, datetime\n",
        "from urllib.parse import urlencode\n",
        "from collections import defaultdict\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.utils import dict_from_cookiejar\n",
        "\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "lPdoU0KzB2G0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Variables modificables**\n",
        "\n",
        "* `planilla_anual`: URL de la planilla de Google Sheets que contiene los datos de consumo. Es un recurso vital para el funcionamiento del script.\n",
        "* **APIs del ERP:** Estas son las claves (`API_KEY` y `API_SECRET`) que se generan en Frappe. Son esenciales para la autenticación y el intercambio de datos con el servidor.\n",
        "* `FRAPPE_API`: Es el dominio o la URL donde está alojado el servidor de Frappe. El script obtiene esta URL de una celda específica de otra planilla de Google Sheets.\n",
        "* `gpu_o_cpu`: Variable para seleccionar si el proceso de OCR se ejecutará en una GPU o en una CPU. Por defecto, está configurada para usar la CPU.\n",
        "* **Otras variables:** El resto de las variables (`archivo_crudo`, `archivo_Series`, etc.) son nombres de archivos temporales que no necesitan ser modificados. Los prefijos válidos (`prefijos_validos`) se utilizan para validar los números de serie detectados por el OCR."
      ],
      "metadata": {
        "id": "1J6kHoXaQrcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ocr con gpu o cpu?\n",
        "gpu_o_cpu = \"C\" #input(\"Para hacer el OCR usamos [G]pu o [C]pu?\")\n",
        "\n",
        "# === ~ Variables ===\n",
        "\n",
        "#crudo\n",
        "planilla_anual = userdata.get('planilla_anual')\n",
        "# URL Apps Script del Forms\n",
        "FormsAppscript = userdata.get('FormsAppscript')\n",
        "\n",
        "# === 🔐 API ERP ===\n",
        "API_KEY = userdata.get('API_KEY')\n",
        "API_SECRET = userdata.get('API_SECRET')\n",
        "FRAPPE_API_crudo = requests.get(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vRbnN152KxnzZqA3GA0L0LQJCF9uGRNIXsydi_FB1EveJ_aVSD_5rjHtTcfoGyctN8ZPou5R7Eg-QQ8/pub?gid=1381182595&single=true&output=csv\").text.splitlines()[1]\n",
        "FRAPPE_API = FRAPPE_API_crudo+\"/api/resource\"\n",
        "\n",
        "\n",
        "# Nombre de los archivos a generar\n",
        "archivo_crudo = \"crudo.csv\" #para el crudo de las OT\n",
        "archivo_Series = \"series_frappe.csv\" #para el listado de series de frappe\n",
        "archivo_productos = \"productos.csv\" #para el listado de productos\n",
        "archivo_ONT = \"columnas_extraidasONT.csv\" #columnas de extraccion de datos ONT\n",
        "archivo_DECO = \"columnas_extraidasDeco.csv\" #columnas de extraccion de datos Decos\n",
        "archivo_consumibles = \"reporte_consumibles.csv\" #cables usados\n",
        "path_viejos=\"url_serieViejo.csv\"\n",
        "path_nuevos=\"series_validas_detectadas.csv\"\n",
        "prefijos_validos = ['4857', 'HP40', 'HP44', 'FZ32', '534D', 'ALCL']"
      ],
      "metadata": {
        "id": "BFhObpzk1mcx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nota de entrega"
      ],
      "metadata": {
        "id": "3buKfC34wHHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def install_other_dependencies():\n",
        "    import subprocess\n",
        "\n",
        "    # --- 1. Verificación del entorno Colab ---\n",
        "    # El módulo google.colab solo existe en ese entorno\n",
        "    try:\n",
        "        import google.colab\n",
        "        IN_COLAB = True\n",
        "        print(\"Detectado: Google Colab. Se ejecutarán comandos de Colab.\")\n",
        "    except ImportError:\n",
        "        IN_COLAB = False\n",
        "        print(\"Detectado: Entorno local.\")\n",
        "\n",
        "    # --- 2. Lógica condicional ---\n",
        "    if IN_COLAB:\n",
        "        # Código específico para Google Colab\n",
        "        print(\"\\n--- Instalando dependencias con comandos de Colab ---\")\n",
        "        !pip install pytesseract\n",
        "        !apt-get install -y tesseract-ocr libtesseract-dev\n",
        "        !pip install paddleocr\n",
        "        # Puedes añadir la lógica para PaddlePaddle aquí\n",
        "        # !pip install paddlepaddle-gpu o !pip install paddlepaddle\n",
        "    else:\n",
        "        # Código específico para un entorno local\n",
        "        try:\n",
        "            # --- Si es Linux, ejecuta los comandos con subprocess ---\n",
        "            print(\"\\n--- Detectado: Linux. Instalando dependencias con apt y pip ---\")\n",
        "\n",
        "            # Comandos de apt-get\n",
        "            print(\"Ejecutando apt-get update...\")\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"update\"], check=True)\n",
        "\n",
        "            print(\"Ejecutando apt-get install tesseract-ocr...\")\n",
        "            subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"tesseract-ocr\", \"libtesseract-dev\"], check=True)\n",
        "\n",
        "            # Comandos de pip\n",
        "            print(\"Instalando librerías de Python con pip...\")\n",
        "            subprocess.run([\"pip\", \"install\", \"pytesseract\"], check=True)\n",
        "            if gpu_o_cpu == \"G\":\n",
        "                subprocess.run([\"pip\", \"install\", \"paddlepaddle-gpu\"], check=True)\n",
        "            else:\n",
        "                subprocess.run([\"pip\", \"install\", \"paddlepaddle\"], check=True)\n",
        "\n",
        "        except:\n",
        "            print(\"\\n--- Sistema operativo no compatible con este script de instalación. ---\")\n",
        "            print(\"Por favor, instala las dependencias manualmente.\")"
      ],
      "metadata": {
        "id": "w2eBwyMBti4s"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funciones de limpieza y estado**\n",
        "\n",
        "* `borrar_archivos_csv()`: Esta función elimina los archivos CSV temporales que se crean durante la ejecución del script. Su propósito es mantener el entorno limpio y evitar que se usen datos de ejecuciones anteriores.\n",
        "\n",
        "* `hay_que_revisar(only_boolean=False)`: Verifica si hay equipos pendientes de transferencia o que no fueron recibidos. Lee los archivos `ONT.csv` y `DECO.csv` para determinar su estado. La función puede devolver un valor booleano (`True` o `False`) si se especifica, o imprimir mensajes detallados sobre los equipos que requieren atención."
      ],
      "metadata": {
        "id": "danUXFUc0MZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def borrar_archivos_csv():\n",
        "    for archivo in [archivo_crudo, archivo_Series, archivo_productos, archivo_ONT, archivo_DECO, archivo_consumibles]:\n",
        "        if os.path.exists(archivo):\n",
        "            os.remove(archivo)\n",
        "            print(f\"🧹 Archivo eliminado: {archivo}\")\n",
        "\n",
        "def hay_que_revisar(only_boolean=False):\n",
        "    def tiene_pendientes(archivo):\n",
        "        with open(archivo, newline='', encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for fila in reader:\n",
        "                if fila.get(\"No recibido\", \"\").strip(): #fila.get(\"Transferencia?\", \"\").strip(): # si tenia transferencias me hacia OCR\n",
        "                    print(fila)\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    pendientes_ont = tiene_pendientes(archivo_ONT)\n",
        "    pendientes_deco = tiene_pendientes(archivo_DECO)\n",
        "\n",
        "    if only_boolean:\n",
        "        return pendientes_ont or pendientes_deco\n",
        "\n",
        "    hay_pendientes = False\n",
        "\n",
        "    if pendientes_ont:\n",
        "        print(f\"⚠️ Aún hay equipos en '{archivo_ONT}' que requieren transferencia o no fueron recibidos.\")\n",
        "        hay_pendientes = True\n",
        "\n",
        "    if pendientes_deco:\n",
        "        print(f\"⚠️ Aún hay equipos en '{archivo_DECO}' que requieren transferencia o no fueron recibidos.\")\n",
        "        hay_pendientes =\n",
        "\n",
        "    if pendientes_ont or pendientes_deco:\n",
        "      print(\"🚫 Revisar equipos no recibidos\")\n",
        "\n",
        "    return hay_pendientes"
      ],
      "metadata": {
        "id": "c66so-4rZcYU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funciones de comunicación con Google Sheets, Forms y Frappe**\n",
        "\n",
        "* `abrirFotosNoRecibidos()`: Esta función, marcada para uso local y no en Colab, extrae las URLs de las fotos de los equipos que no se han recibido. Su propósito es preparar los enlaces para su posterior descarga o visualización.\n",
        "\n",
        "* `enviarOTForms(ot_number)`: Envía un número de Orden de Trabajo (`ot_number`) a una aplicación web de Google Apps Script. Esto se utiliza para registrar o notificar la finalización de una OT.\n",
        "\n",
        "* `ultimaOTconsumida()`: Lee la última fila del archivo `crudo.csv` para encontrar el número de la última Orden de Trabajo procesada y luego llama a `enviarOTForms()` para enviar este número a la aplicación de Google Forms.\n",
        "\n",
        "* `agregar_comentario()`: Se conecta a la API de Frappe para añadir un comentario a un documento específico. Recibe el tipo y nombre del documento, así como el contenido del comentario, y maneja de manera robusta los errores de conexión o de la API."
      ],
      "metadata": {
        "id": "ovJroQyw0pWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def abrirFotosNoRecibidos(): # sin usar si es en colab\n",
        "    archivos = [archivo_ONT, archivo_DECO]\n",
        "    campo_fotos = \"foto (no recibido)\"\n",
        "    urls = set()\n",
        "\n",
        "    for archivo in archivos:\n",
        "        try:\n",
        "            with open(archivo, newline='', encoding=\"utf-8\") as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                for fila in reader:\n",
        "                    fotos_raw = fila.get(campo_fotos, \"\").strip()\n",
        "                    if fotos_raw:\n",
        "                        for link in fotos_raw.split(\"\\n\"):\n",
        "                            url = link.strip()\n",
        "                            if url.startswith(\"http\"):\n",
        "                                urls.add(url)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ No se pudo procesar {archivo}: {e}\")\n",
        "\n",
        "def enviarOTForms(ot_number):\n",
        "    \"\"\"\n",
        "    Envía un número de OT a la Web App de Google Apps Script.\n",
        "\n",
        "    Args:\n",
        "        ot_number (str): El número de OT que se enviará.\n",
        "\n",
        "    Returns:\n",
        "        requests.Response: El objeto de respuesta de la petición.\n",
        "    \"\"\"\n",
        "        # Datos que se enviarán en el cuerpo de la petición (formato JSON)\n",
        "    payload = {\n",
        "        \"otNumber\": ot_number\n",
        "    }\n",
        "\n",
        "    # Cabeceras para indicar que el contenido es JSON\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Realizar la petición POST\n",
        "        response = requests.post(FormsAppscript, data=json.dumps(payload), headers=headers)\n",
        "\n",
        "        # Verificar si la petición fue exitosa (código de estado 200)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"Enviado a Forms\")\n",
        "        else:\n",
        "            print(f\"Error en la petición. Código de estado: {response.status_code}\")\n",
        "            print(f\"Respuesta del servidor: {response.text}\")\n",
        "\n",
        "        return response\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Se produjo un error al conectar con la Web App: {e}\")\n",
        "        return None\n",
        "\n",
        "def ultimaOTconsumida():\n",
        "    archivo = archivo_crudo\n",
        "    ultima_fila = None\n",
        "    with open(archivo, newline='', encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        for fila in reader:\n",
        "            ultima_fila = fila\n",
        "    if ultima_fila and len(ultima_fila) > 5:\n",
        "        print()\n",
        "        print()\n",
        "        print(f\"Última OT del {archivo}: {ultima_fila[5]}\")\n",
        "        enviarOTForms(ultima_fila[5])\n",
        "        print()\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"No se pudo leer la OT en la última fila de '{archivo}'.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def agregar_comentario(doctype: str, docname: str, contenido: str, remitente: str = \"aguida@qpsrl.com.ar\") -> dict:\n",
        "    \"\"\"\n",
        "    Agrega un comentario a un documento en Frappe utilizando las API Keys y endpoint predefinidos.\n",
        "\n",
        "    Args:\n",
        "        doctype (str): El tipo de documento de referencia en Frappe (ej: \"Stock Entry\", \"Delivery Note\").\n",
        "        docname (str): El nombre o ID del documento en Frappe (ej: \"MAT-STE-2025-01263\").\n",
        "        contenido (str): El contenido del comentario, se espera en formato HTML (ej: \"<p>Mi comentario</p>\").\n",
        "        remitente (str, optional): Email del remitente. Por defecto es \"aguida@qpsrl.com.ar\".\n",
        "\n",
        "    Returns:\n",
        "        dict: Resultado de la operación, indicando éxito, errores y la respuesta de Frappe.\n",
        "    \"\"\"\n",
        "    if not API_KEY or not API_SECRET:\n",
        "        print(\"Error: API credentials (API_KEY or API_SECRET) are not set.\")\n",
        "        return {\"success\": False, \"error\": \"API credentials not found\"}\n",
        "\n",
        "    if not FRAPPE_API_crudo:\n",
        "        print(\"Error: Frappe API endpoint (FRAPPE_API_crudo) is not set. Check CSV fetch.\")\n",
        "        return {\"success\": False, \"error\": \"Frappe API endpoint not found\"}\n",
        "\n",
        "    try:\n",
        "        # Usar el método add_comment de Frappe para una integración más robusta\n",
        "        url = f\"{FRAPPE_API_crudo}/api/method/frappe.desk.form.utils.add_comment\"\n",
        "\n",
        "        payload = {\n",
        "            \"reference_doctype\": doctype,\n",
        "            \"reference_name\": docname,\n",
        "            \"content\": contenido,\n",
        "            \"comment_email\": remitente,\n",
        "            \"comment_by\": remitente.split(\"@\")[0] # Utiliza la parte antes del @ como nombre\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"Authorization\": f\"token {API_KEY}:{API_SECRET}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Accept\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        response.raise_for_status()  # Lanza una excepción para errores HTTP (4xx o 5xx)\n",
        "\n",
        "        response_data = response.json()\n",
        "\n",
        "        print(f\"🗨️ Comentario agregado a {docname} ({doctype}) usando add_comment\")\n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"doctype\": doctype,\n",
        "            \"docname\": docname,\n",
        "            \"comment\": contenido,\n",
        "            \"response\": response_data\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # Captura errores específicos de requests (conexión, HTTP, etc.)\n",
        "        error_message = f\"Request Error: {e}\"\n",
        "        if response.status_code:\n",
        "            error_message += f\" (HTTP {response.status_code}: {response.text})\"\n",
        "        print(f\"❌ Error al enviar comentario a Frappe: {error_message}\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": error_message,\n",
        "            \"doctype\": doctype,\n",
        "            \"docname\": docname\n",
        "        }\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Captura errores si la respuesta no es JSON válido\n",
        "        error_message = f\"JSON Decode Error: {e}. Response text: {response.text}\"\n",
        "        print(f\"❌ Error al procesar la respuesta JSON: {error_message}\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": error_message,\n",
        "            \"doctype\": doctype,\n",
        "            \"docname\": docname\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # Captura cualquier otro error inesperado\n",
        "        print(f\"❌ Error inesperado en agregar_comentario: {e}\")\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": str(e),\n",
        "            \"doctype\": doctype,\n",
        "            \"docname\": docname\n",
        "        }"
      ],
      "metadata": {
        "id": "co5BXNjx0vF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funciones de obtención de datos y filtrado básico**\n",
        "\n",
        "* `obtenerCrudo()`: Esta función se encarga de descargar los datos directamente de una planilla de Google Sheets. Filtra las filas que ya han sido procesadas, añade la etiqueta \" - QPS\" a los nombres de los técnicos si es necesario, y guarda los datos limpios en un archivo CSV local para su posterior uso. Si no encuentra filas pendientes, termina el script para evitar procesamientos innecesarios.\n",
        "\n",
        "* `obtenerSeriesFrappe()`: Se comunica con la API de Frappe para obtener una lista de todos los números de serie (`Serial No`) que se encuentran en el sistema. Los guarda en un archivo CSV (`Series.csv`) para poder comparar y validar los datos de la planilla con el inventario del ERP.\n",
        "\n",
        "* `obtenerProductos()`: Descarga un reporte de \"Stock Projected Qty\" desde la API de Frappe. Este reporte contiene información sobre la cantidad de artículos en cada almacén. La función procesa esta respuesta y guarda los datos en un archivo CSV (`productos.csv`), lo que es esencial para realizar verificaciones de stock antes de crear las notas de entrega."
      ],
      "metadata": {
        "id": "VdCfxzQcZi8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obtenerCrudo():\n",
        "    print(\"Descargando CSV...\")\n",
        "    resp = requests.get(planilla_anual)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    lineas = resp.content.decode(\"utf-8\").splitlines()\n",
        "\n",
        "    print(\"Filtrando filas que no hayan sido consumidas...\")\n",
        "    reader = csv.reader(lineas, delimiter=',')\n",
        "    filas_filtradas = []\n",
        "\n",
        "    for fila in reader:\n",
        "        if len(fila) > 25 and fila[4].strip() == \"\" and ( # acá en podes cambiar el de fila 4 a Si por si hay algun serie que quedó en el limbo\n",
        "            fila[23].strip() == \"EXITOSA\" or\n",
        "            fila[23].strip() == \"CONTINGENCIA / SUSPENDIDA\"\n",
        "        ):\n",
        "            # Columna D (índice 3) es el técnico. Le agregamos \" - QPS\" si no lo tiene.\n",
        "            tecnico = fila[3].strip()\n",
        "            if tecnico and not tecnico.endswith(\" - QPS\"):\n",
        "                fila[3] = f\"{tecnico} - QPS\"\n",
        "            filas_filtradas.append(fila)\n",
        "\n",
        "    if not filas_filtradas:\n",
        "        print(\"🚫 No hay filas pendientes de consumo. Terminando ejecución.\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    with open(archivo_crudo, \"w\", newline='', encoding=\"utf-8\") as f_out:\n",
        "        writer = csv.writer(f_out, delimiter=',')\n",
        "        writer.writerows(filas_filtradas)\n",
        "\n",
        "    print(f\"✅ Filtrado completo. {len(filas_filtradas)} filas guardadas en '{archivo_crudo}'\")\n",
        "\n",
        "\n",
        "def obtenerSeriesFrappe():\n",
        "    print(\"Consultando series en Frappe...\")\n",
        "\n",
        "    FRAPPE_API_serie = FRAPPE_API+\"/Serial%20No\"\n",
        "    HEADERS = {\n",
        "        \"Authorization\": f\"token {API_KEY}:{API_SECRET}\"\n",
        "    }\n",
        "\n",
        "    campos = [\n",
        "        \"name\", \"docstatus\", \"item_code\", \"item_name\",\n",
        "        \"warehouse\", \"creation\", \"modified\", \"_user_tags\"\n",
        "    ]\n",
        "\n",
        "    fields_param = json.dumps(campos)  # ✅ Convierte a JSON válido\n",
        "    url = f\"{FRAPPE_API_serie}?fields={fields_param}&limit_page_length=1000\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=HEADERS)\n",
        "        response.raise_for_status()\n",
        "        data = response.json().get(\"data\", [])\n",
        "\n",
        "        with open(archivo_Series, \"w\", newline='', encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"sr\"] + campos)  # encabezados\n",
        "\n",
        "            for i, fila in enumerate(data, 1):\n",
        "                writer.writerow([\n",
        "                    i,\n",
        "                    fila.get(\"name\", \"\"),\n",
        "                    fila.get(\"docstatus\", \"\"),\n",
        "                    fila.get(\"item_code\", \"\"),\n",
        "                    fila.get(\"item_name\", \"\"),\n",
        "                    fila.get(\"warehouse\", \"\"),\n",
        "                    fila.get(\"creation\", \"\"),\n",
        "                    fila.get(\"modified\", \"\"),\n",
        "                    fila.get(\"_user_tags\", \"\")\n",
        "                ])\n",
        "\n",
        "        print(f\"✅ {len(data)} series guardadas en '{archivo_Series}'\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Error al hacer la solicitud a Frappe: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def obtenerProductos():\n",
        "    \"\"\"Función principal para obtener el stock proyectado\"\"\"\n",
        "    print(\"Iniciando descarga de stock proyectado...\")\n",
        "\n",
        "    # Configuración de headers que funciona\n",
        "    headers = {\n",
        "        \"Authorization\": f\"token {API_KEY}:{API_SECRET}\",\n",
        "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
        "        \"ngrok-skip-browser-warning\": \"true\",\n",
        "        \"X-Frappe-CSRF-Token\": \"None\"\n",
        "    }\n",
        "\n",
        "    # Payload que funciona (según pruebas)\n",
        "    payload = {\n",
        "        \"cmd\": \"frappe.desk.query_report.run\",\n",
        "        \"report_name\": \"Stock Projected Qty\",\n",
        "        \"filters\": json.dumps({\"company\": \"Quality Plus\", \"ignore_permissions\": 1}),\n",
        "        \"limit\": \"1000\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Enviar solicitud\n",
        "        response = requests.post(\n",
        "            f\"{FRAPPE_API}/api/method/frappe.desk.query_report.run\",\n",
        "            headers=headers,\n",
        "            data=urlencode(payload)\n",
        "        ) # Added the missing closing parenthesis here\n",
        "\n",
        "        # Verificar respuesta\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if not data.get(\"message\") or not isinstance(data[\"message\"].get(\"result\"), list):\n",
        "            raise Exception(\"La respuesta no contiene datos válidos\")\n",
        "\n",
        "        resultados = data[\"message\"][\"result\"]\n",
        "        print(f\"Se obtuvieron {len(resultados)} registros de stock\")\n",
        "\n",
        "        # Columnas fijas basadas en el reporte Stock Projected Qty\n",
        "        columnas = [\n",
        "            \"item_code\", \"item_name\",\"warehouse\", \"actual_qty\"\n",
        "        ]\n",
        "\n",
        "        # Guardar en CSV\n",
        "        with open(archivo_productos, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=columnas, extrasaction='ignore')\n",
        "            writer.writeheader()\n",
        "\n",
        "            for row in resultados:\n",
        "                # Asegurarnos de que sea un diccionario\n",
        "                if isinstance(row, dict):\n",
        "                    writer.writerow(row)\n",
        "                else:\n",
        "                    # Si es una lista, convertir a diccionario con las columnas conocidas\n",
        "                    if isinstance(row, list) and len(row) == len(columnas):\n",
        "                        writer.writerow(dict(zip(columnas, row)))\n",
        "                    #else:\n",
        "                    #    print(f\"⚠️ Formato de fila no reconocido: {row}\")\n",
        "\n",
        "        print(f\"✅ Datos guardados correctamente en {archivo_productos}\")\n",
        "        return True\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ Error en la conexión: {str(e)}\")\n",
        "        if hasattr(e, 'response') and e.response:\n",
        "            print(f\"Respuesta del servidor: {e.response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error al procesar los datos: {str(e)}\")\n",
        "        if 'data' in locals() and data.get('message') and data['message'].get('result'):\n",
        "             print(f\"Tipo de datos recibidos: {type(data['message']['result'][0])}\")\n",
        "        else:\n",
        "             print(\"Tipo de datos recibidos: N/A\")\n",
        "\n",
        "\n",
        "    return False"
      ],
      "metadata": {
        "id": "gt82fPMtZpxt"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funciones de filtrado y categorización**\n",
        "\n",
        "* `extraer_columnas_ont()`: Esta función lee los datos de la planilla (`crudo.csv`) y los de los números de serie (`series_frappe.csv`) para procesar los equipos ONT (modems de fibra). Filtra las filas de \"instalación, cambio de domicilio\" y determina si un equipo fue recibido, si requiere una transferencia de stock o si debe ser consumido en la nota de entrega. El resultado se guarda en `columnas_extraidasONT.csv`.\n",
        "\n",
        "* `extraer_columnas_deco()`: Similar a la función anterior, pero se enfoca en los decodificadores. Procesa las filas del archivo `crudo.csv` que contienen series de Decos. Esta función también verifica el estado de cada Deco (si requiere transferencia, fue recibido o debe consumirse) y guarda la información en un archivo CSV específico para Decos (`columnas_extraidasDeco.csv`).\n",
        "\n",
        "* `extraer_consumibles()`: Esta función es responsable de calcular el consumo de consumibles (como cables y conectores) basándose en los datos de las OTs y el stock disponible. Carga la información de stock desde `productos.csv` y aplica una lógica específica para ciertos ítems, ajustando el consumo solicitado con el stock real disponible. Finalmente, genera un informe detallado de los consumos finales en `reporte_consumibles.csv`."
      ],
      "metadata": {
        "id": "uNt_SmYWZvGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_columnas_ont():\n",
        "    archivo_entrada = archivo_crudo\n",
        "    archivo_salida = archivo_ONT\n",
        "\n",
        "    encabezados = [\n",
        "        \"ONT (modem fibra)\",\n",
        "        \"foto\",\n",
        "        \"Cargó OT\",\n",
        "        \"Origen equipo\",\n",
        "        \"Transferencia?\",\n",
        "        \"No recibido\",\n",
        "        \"foto (no recibido)\",\n",
        "        \"Consumir\"\n",
        "    ]\n",
        "\n",
        "    # Leer series_frappe.csv y construir un diccionario {serie: warehouse}\n",
        "    dic_ont_origen = {}\n",
        "    with open(archivo_Series, newline='', encoding=\"utf-8\") as f_series:\n",
        "        reader_series = csv.reader(f_series)\n",
        "        next(reader_series)  # saltar encabezado\n",
        "        for fila in reader_series:\n",
        "            if len(fila) > 5:\n",
        "                serie = fila[1].strip()\n",
        "                warehouse = fila[5].strip()\n",
        "                dic_ont_origen[serie] = warehouse\n",
        "\n",
        "    # Leer crudo.csv y procesar\n",
        "    with open(archivo_entrada, newline='', encoding=\"utf-8\") as f_in:\n",
        "        reader = csv.reader(f_in)\n",
        "        filas_salida = []\n",
        "\n",
        "        for fila in reader:\n",
        "            if len(fila) <= 9:\n",
        "                continue  # ignorar filas mal formateadas\n",
        "\n",
        "            # Verificar si en columna Z (índice 26 - 1 = 25) está \"INSTALACION CAMBIO DE DOMICILIO\"\n",
        "            if len(fila) > 25 and fila[25].strip().upper() == \"INSTALACION CAMBIO DE DOMICILIO\":\n",
        "                continue  # saltar esta fila\n",
        "\n",
        "            ont = fila[9].strip()\n",
        "            if not ont:\n",
        "                continue\n",
        "\n",
        "            foto = fila[8].strip()\n",
        "            cargo_ot_base = fila[3].strip()\n",
        "            cargo_ot = f\"{cargo_ot_base}\" if cargo_ot_base else \"\"\n",
        "\n",
        "            # Determinar origen equipo\n",
        "            if ont in dic_ont_origen and dic_ont_origen[ont].strip():\n",
        "                origen_equipo = dic_ont_origen[ont].strip()\n",
        "            elif ont not in dic_ont_origen:\n",
        "                origen_equipo = \"No\"\n",
        "            else:\n",
        "                origen_equipo = \"\"\n",
        "\n",
        "            # Transferencia (solo si origen_equipo no es \"No\")\n",
        "            if origen_equipo and origen_equipo != cargo_ot and origen_equipo != \"No\":\n",
        "                transferencia = f\"{origen_equipo}, {cargo_ot}\"\n",
        "            else:\n",
        "                transferencia = \"\"\n",
        "\n",
        "            no_recibido = \"\"\n",
        "            foto_no_recibido = \"\"\n",
        "\n",
        "            if origen_equipo == \"No\":\n",
        "                no_recibido = ont\n",
        "                foto_no_recibido = \"\\n\".join([f.strip() for f in foto.split(\",\") if f.strip()])\n",
        "\n",
        "            consumir = \"\"\n",
        "            if origen_equipo != \"No\" and origen_equipo == cargo_ot:\n",
        "                consumir = ont\n",
        "\n",
        "            filas_salida.append([\n",
        "                ont,\n",
        "                foto,\n",
        "                cargo_ot,\n",
        "                origen_equipo,\n",
        "                transferencia,\n",
        "                no_recibido,\n",
        "                foto_no_recibido,\n",
        "                consumir\n",
        "            ])\n",
        "\n",
        "    with open(archivo_salida, \"w\", newline='', encoding=\"utf-8\") as f_out:\n",
        "        writer = csv.writer(f_out)\n",
        "        writer.writerow(encabezados)\n",
        "        writer.writerows(filas_salida)\n",
        "\n",
        "    print(f\"✅ Archivo '{archivo_salida}' generado con {len(filas_salida)} filas filtradas.\")\n",
        "\n",
        "\n",
        "def extraer_columnas_deco():\n",
        "    archivo_entrada = archivo_crudo\n",
        "    archivo_salida = archivo_DECO\n",
        "\n",
        "    encabezados = [\n",
        "        \"Deco\",\n",
        "        \"foto\",\n",
        "        \"Cargó OT\",\n",
        "        \"Origen equipo\",\n",
        "        \"Transferencia?\",\n",
        "        \"No recibido\",\n",
        "        \"foto (no recibido)\",\n",
        "        \"Consumir\"\n",
        "    ]\n",
        "\n",
        "    # Leer series_frappe.csv y construir un diccionario {serie: warehouse}\n",
        "    dic_deco_origen = {}\n",
        "    with open(archivo_Series, newline='', encoding=\"utf-8\") as f_series:\n",
        "        reader_series = csv.reader(f_series)\n",
        "        next(reader_series)  # saltar encabezado\n",
        "        for fila in reader_series:\n",
        "            if len(fila) > 5:\n",
        "                serie = fila[1].strip()\n",
        "                warehouse = fila[5].strip()\n",
        "                dic_deco_origen[serie] = warehouse\n",
        "\n",
        "    # Leer crudo.csv y procesar\n",
        "    with open(archivo_entrada, newline='', encoding=\"utf-8\") as f_in:\n",
        "        reader = csv.reader(f_in)\n",
        "        filas_salida = []\n",
        "\n",
        "        for fila in reader:\n",
        "            # ❗ Ignorar si columna AB (índice 27) es \"INSTALACION CAMBIO DE DOMICILIO\"\n",
        "            if len(fila) > 27 and fila[27].strip().upper() == \"INSTALACION CAMBIO DE DOMICILIO\":\n",
        "                continue\n",
        "\n",
        "            if len(fila) > 10 and fila[10].strip() != \"\":\n",
        "                deco_series = fila[10].strip().split()  # Puede haber múltiples series separados por espacio\n",
        "                foto = fila[8].strip()\n",
        "                cargo_ot_base = fila[3].strip()\n",
        "                cargo_ot = f\"{cargo_ot_base}\" if cargo_ot_base else \"\"\n",
        "\n",
        "                for deco in deco_series:\n",
        "                    origen_equipo = \"No\" # esto es por si no se recibió el equipo\n",
        "                    if deco in dic_deco_origen and dic_deco_origen[deco].strip():\n",
        "                        origen_equipo = dic_deco_origen[deco].strip()\n",
        "                    elif deco not in dic_deco_origen:\n",
        "                        origen_equipo = \"No\"\n",
        "                    else:\n",
        "                        origen_equipo = \"\"\n",
        "\n",
        "                    # Transferencia (solo si origen_equipo no es \"No\")\n",
        "                    if origen_equipo and origen_equipo != cargo_ot and origen_equipo != \"No\":\n",
        "                        transferencia = f\"{origen_equipo}, {cargo_ot}\"\n",
        "                    else:\n",
        "                        transferencia = \"\"\n",
        "\n",
        "                    no_recibido = \"\"\n",
        "                    foto_no_recibido = \"\"\n",
        "                    if origen_equipo == \"No\":\n",
        "                        no_recibido = deco\n",
        "                        foto_no_recibido = \"\\n\".join([f.strip() for f in foto.split(\",\") if f.strip()])\n",
        "\n",
        "                    # Consumir: si no requiere transferencia y no tiene origen \"No\"\n",
        "                    consumir = \"\"\n",
        "                    if origen_equipo != \"No\": # and origen_equipo == cargo_ot: # si el equipo se recibió y el origen es el que lo cargó\n",
        "                        consumir = deco\n",
        "\n",
        "                    filas_salida.append([\n",
        "                        deco,\n",
        "                        foto,\n",
        "                        cargo_ot,\n",
        "                        origen_equipo,\n",
        "                        transferencia,\n",
        "                        no_recibido,\n",
        "                        foto_no_recibido,\n",
        "                        consumir\n",
        "                    ])\n",
        "\n",
        "    with open(archivo_salida, \"w\", newline='', encoding=\"utf-8\") as f_out:\n",
        "        writer = csv.writer(f_out)\n",
        "        writer.writerow(encabezados)\n",
        "        writer.writerows(filas_salida)\n",
        "\n",
        "    print(f\"✅ Archivo '{archivo_salida}' generado con {len(filas_salida)} filas.\")\n",
        "\n",
        "def extraer_consumibles():\n",
        "    # Asegúrate de que las variables globales sean accesibles\n",
        "    global archivo_consumibles, archivo_productos, archivo_crudo, archivo_DECO\n",
        "\n",
        "    archivo_salida = archivo_consumibles\n",
        "    item_code_fijo = \"20710003\" # Ejemplo: Cable UTP 1 metro\n",
        "    item_code_extra1 = \"110100372\" # Ejemplo: Conector SC/APC\n",
        "    item_code_extra2 = \"320500065\" # Ejemplo: Pigtail SC/APC\n",
        "\n",
        "    # Ítems que se consumen \"por completo según stock total\" (independientemente de la cantidad solicitada)\n",
        "    ITEMS_CONSUMIR_TODO_STOCK = {\n",
        "        \"140110033\", \"170200005\", \"170200010\",\n",
        "        \"170300004\", \"20310001\", \"20710010\"\n",
        "    }\n",
        "\n",
        "    print(\"🛠️ Iniciando extracción y cálculo de consumibles...\")\n",
        "\n",
        "    # 1. Cargar diccionario de productos y stock disponible\n",
        "    # productos: item_code -> item_name\n",
        "    # stock_disponible: item_code -> warehouse -> actual_qty (float)\n",
        "    productos = {}\n",
        "    stock_disponible = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    if os.path.exists(archivo_productos):\n",
        "        with open(archivo_productos, newline='', encoding=\"utf-8\") as f_stock:\n",
        "            reader = csv.DictReader(f_stock)\n",
        "            for row in reader:\n",
        "                codigo = row.get(\"item_code\", \"\").strip()\n",
        "                nombre = row.get(\"item_name\", \"\").strip()\n",
        "                almacen = row.get(\"warehouse\", \"\").strip()\n",
        "                try:\n",
        "                    stock = float(row.get(\"actual_qty\", 0))\n",
        "                except ValueError:\n",
        "                    stock = 0.0\n",
        "                    print(f\"⚠️ Stock no numérico para {codigo} en {almacen}. Asumiendo 0.\")\n",
        "\n",
        "                if codigo and almacen: # Asegurarse de que tengamos un código y un almacén válidos\n",
        "                    productos[codigo] = nombre\n",
        "                    stock_disponible[codigo][almacen] = stock\n",
        "    else:\n",
        "        print(f\"❌ Advertencia: Archivo '{archivo_productos}' no encontrado. No se aplicará control de stock para NINGÚN ítem.\")\n",
        "        # Si el archivo de stock no existe, todos los ítems se tratarán como si no tuvieran stock para control,\n",
        "        # lo que significa que solo se consumirán si NO están en ITEMS_CONSUMIR_TODO_STOCK (y por lo tanto, su stock será 0).\n",
        "\n",
        "    # 2. Contar ocurrencias por (item_code, warehouse) de lo que se NECESITA consumir\n",
        "    # conteo_solicitado: item_code -> warehouse -> qty_solicitada (int)\n",
        "    conteo_solicitado = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    # Procesar archivo_crudo (columna N: cables, columna O: cantidad fija)\n",
        "    if os.path.exists(archivo_crudo):\n",
        "        with open(archivo_crudo, newline='', encoding=\"utf-8\") as f_crudo:\n",
        "            reader = csv.reader(f_crudo)\n",
        "            for fila in reader:\n",
        "                if len(fila) < 15: # Necesitamos hasta la columna O (índice 14)\n",
        "                    continue\n",
        "\n",
        "                campo_cable = fila[13].strip() # Columna N\n",
        "                campo_consumible = fila[14].strip() # Columna O\n",
        "                cargo_ot_base = fila[3].strip() # Columna D (Técnico / Almacén)\n",
        "                warehouse = cargo_ot_base if cargo_ot_base else \"Default Warehouse\"\n",
        "\n",
        "                # 🔹 Cables (de columna N)\n",
        "                if \" - \" in campo_cable:\n",
        "                    item_code_cable = campo_cable.split(\" - \")[0].strip()\n",
        "                    if item_code_cable:\n",
        "                        conteo_solicitado[item_code_cable][warehouse] += 1\n",
        "\n",
        "                # 🔹 Cantidades para item_code fijo (de columna O)\n",
        "                if campo_consumible:\n",
        "                    try:\n",
        "                        cantidad_fija = int(campo_consumible)\n",
        "                        if cantidad_fija > 0:\n",
        "                            conteo_solicitado[item_code_fijo][warehouse] += cantidad_fija\n",
        "                    except ValueError:\n",
        "                        print(f\"⚠️ Valor no numérico en columna O de '{archivo_crudo}' para {warehouse}: '{campo_consumible}'. Ignorado.\")\n",
        "    else:\n",
        "        print(f\"❌ Advertencia: Archivo '{archivo_crudo}' no encontrado. No se procesarán consumibles del crudo.\")\n",
        "\n",
        "    # Procesar serializados desde archivo_DECO\n",
        "    if os.path.exists(archivo_DECO):\n",
        "        with open(archivo_DECO, newline='', encoding=\"utf-8\") as f_deco:\n",
        "            reader = csv.DictReader(f_deco)\n",
        "            for fila in reader:\n",
        "                serie = fila.get(\"Consumir\", \"\").strip()\n",
        "                almacen = fila.get(\"Cargó OT\", \"\").strip()\n",
        "\n",
        "                if serie and almacen:\n",
        "                    warehouse_deco = almacen\n",
        "                    conteo_solicitado[item_code_extra1][warehouse_deco] += 1\n",
        "                    conteo_solicitado[item_code_extra2][warehouse_deco] += 2\n",
        "    else:\n",
        "        print(f\"❌ Advertencia: Archivo '{archivo_DECO}' no encontrado. No se procesarán serializados de DECO.\")\n",
        "\n",
        "    # 3. Calcular consumos finales aplicando la lógica de stock\n",
        "    # consumos_finales: item_code -> warehouse -> {'solicitada': X, 'real': Y}\n",
        "    consumos_finales = defaultdict(lambda: defaultdict(lambda: {'solicitada': 0, 'real': 0}))\n",
        "\n",
        "    # (Lógica original de procesamiento de ítems solicitados)\n",
        "    for item_code, almacenes_solicitados in conteo_solicitado.items():\n",
        "        for warehouse, qty_solicitada in almacenes_solicitados.items():\n",
        "            qty_consumida_real = 0\n",
        "\n",
        "            stock_actual_item_wh = stock_disponible.get(item_code, {}).get(warehouse, 0.0)\n",
        "            if item_code not in productos and stock_disponible.get(item_code):\n",
        "                # Se podría buscar el nombre en otro lado si se necesita,\n",
        "                # pero por ahora no es crucial para la lógica de stock.\n",
        "                pass\n",
        "\n",
        "            if item_code in ITEMS_CONSUMIR_TODO_STOCK:\n",
        "                # El problema es que esta lógica solo se ejecuta si el ítem está en `conteo_solicitado`.\n",
        "                # La solución es mover el manejo de ITEMS_CONSUMIR_TODO_STOCK a una nueva sección\n",
        "                # para que se procesen incluso si no fueron explícitamente solicitados.\n",
        "                # Aquí, la lógica original está causando que solo se consuman si se solicitan.\n",
        "                # Lo dejaré como estaba para mostrar que hay que cambiarlo\n",
        "                qty_consumida_real = int(stock_actual_item_wh)\n",
        "                # ... (los print originales) ...\n",
        "            else:\n",
        "                # El resto de los ítems (control estricto de solicitado vs. disponible)\n",
        "                qty_consumida_real = min(qty_solicitada, int(stock_actual_item_wh))\n",
        "                # ... (los print originales) ...\n",
        "\n",
        "            consumos_finales[item_code][warehouse]['solicitada'] = qty_solicitada\n",
        "            consumos_finales[item_code][warehouse]['real'] = qty_consumida_real\n",
        "\n",
        "            stock_disponible[item_code][warehouse] -= qty_consumida_real\n",
        "            if stock_disponible[item_code][warehouse] < 0.00001:\n",
        "                stock_disponible[item_code][warehouse] = 0.0\n",
        "\n",
        "    # --- manejar ITEMS_CONSUMIR_TODO_STOCK ---\n",
        "    print(\"\\n🔄 Procesando ítems de 'consumir todo el stock'...\")\n",
        "    for item_code_consumir in ITEMS_CONSUMIR_TODO_STOCK:\n",
        "        # Solo procesar si el ítem existe en nuestro diccionario de stock\n",
        "        if item_code_consumir in stock_disponible:\n",
        "            for warehouse, stock_actual in stock_disponible[item_code_consumir].items():\n",
        "                # Verifica que el almacén comience con \"51\"\n",
        "                if warehouse.startswith(\"51\"):\n",
        "                    # Si no se solicitó (el valor 'real' es 0), lo actualizamos aquí.\n",
        "                    if consumos_finales[item_code_consumir][warehouse]['real'] == 0 and stock_actual > 0:\n",
        "                        qty_consumida_real = int(stock_actual)\n",
        "\n",
        "                        # Registrar en consumos_finales\n",
        "                        # La cantidad solicitada se deja en 0 ya que no hubo una solicitud explícita\n",
        "                        consumos_finales[item_code_consumir][warehouse]['solicitada'] = 0\n",
        "                        consumos_finales[item_code_consumir][warehouse]['real'] = qty_consumida_real\n",
        "\n",
        "                        print(f\"✅ Consumiendo {qty_consumida_real} de {item_code_consumir} en {warehouse} (ítem de consumo total).\")\n",
        "\n",
        "    # 4. Generar archivo de salida con los consumos finales\n",
        "    filas_totales = 0\n",
        "    with open(archivo_salida, \"w\", newline='', encoding=\"utf-8\") as f_out:\n",
        "        writer = csv.writer(f_out)\n",
        "        writer.writerow([\"item_code\", \"item_name\", \"warehouse\", \"qty_solicitada\", \"qty\"])\n",
        "\n",
        "        for item_code, almacenes_finales in consumos_finales.items():\n",
        "            item_name = productos.get(item_code, f\"❓ DESCONOCIDO ({item_code})\")\n",
        "            for warehouse, qtys in almacenes_finales.items():\n",
        "                qty_solicitada_original = qtys['solicitada']\n",
        "                qty_consumida_real = qtys['real']\n",
        "\n",
        "                # Solo escribir si se solicitó o se consumió algo.\n",
        "                if qty_solicitada_original > 0 or qty_consumida_real > 0:\n",
        "                    writer.writerow([\n",
        "                        item_code,\n",
        "                        item_name,\n",
        "                        warehouse,\n",
        "                        qty_solicitada_original,\n",
        "                        qty_consumida_real\n",
        "                    ])\n",
        "                    filas_totales += 1\n",
        "\n",
        "    print(f\"✅ Archivo '{archivo_salida}' generado con {filas_totales} filas (consumos ajustados por stock y lista ITEMS_CONSUMIR_TODO_STOCK).\")\n",
        "    print(\"✨ Proceso de extracción y cálculo de consumibles completado.\")"
      ],
      "metadata": {
        "id": "IqZgELFVvUcp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Procesos previos a la Delivery Note**\n",
        "\n",
        "* `transferirPendientes()`: Esta función revisa los equipos marcados para transferencia en los archivos CSV (`columnas_extraidasONT.csv` y `columnas_extraidasDeco.csv`). Si encuentra un equipo que necesita ser transferido de un técnico a otro, se comunica con la API de Frappe para crear un `Stock Entry` (movimiento de stock). Esto asegura que el stock esté en el almacén correcto antes de intentar generar la nota de entrega. También maneja posibles errores de la API e intenta una solución alternativa en caso de fallos."
      ],
      "metadata": {
        "id": "GrnpjLJ3vd7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transferirPendientes():\n",
        "    if hay_que_revisar():\n",
        "        return\n",
        "\n",
        "    # Cargar mapa {serie: item_code}\n",
        "    def construir_diccionario_series():\n",
        "        dic = {}\n",
        "        with open(\"series_frappe.csv\", newline='', encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                serie = row[\"name\"].strip()\n",
        "                item_code = row[\"item_code\"].strip()\n",
        "                if serie and item_code:\n",
        "                    dic[serie] = item_code\n",
        "        return dic\n",
        "\n",
        "    HEADERS = {\n",
        "        \"Authorization\": f\"token {API_KEY}:{API_SECRET}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Accept\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    archivos = [\n",
        "        (\"columnas_extraidasONT.csv\", \"ONT (modem fibra)\"),\n",
        "        (\"columnas_extraidasDeco.csv\", \"Deco\")\n",
        "    ]\n",
        "    url_frappe = FRAPPE_API + \"/Stock%20Entry\"\n",
        "\n",
        "    # Construir mapa serie -> item_code\n",
        "    serie_to_item = construir_diccionario_series()\n",
        "\n",
        "    transferencias_por_tecnico = defaultdict(list)\n",
        "\n",
        "    print(\"🚚 Procesando transferencias pendientes...\")\n",
        "\n",
        "    # Configurar sesión con reintentos básicos\n",
        "    session = requests.Session()\n",
        "    adapter = requests.adapters.HTTPAdapter(max_retries=3)\n",
        "    session.mount(\"http://\", adapter)\n",
        "    session.mount(\"https://\", adapter)\n",
        "\n",
        "    for archivo, campo_serie in archivos:\n",
        "        with open(archivo, newline='', encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for fila in reader:\n",
        "                serie = fila.get(campo_serie, \"\").strip()\n",
        "                transferencia = fila.get(\"Transferencia?\", \"\").strip()\n",
        "\n",
        "                if not serie or not transferencia or \",\" not in transferencia:\n",
        "                    continue\n",
        "\n",
        "                origen, destino = [x.strip() for x in transferencia.split(\",\", 1)]\n",
        "                item_code = serie_to_item.get(serie)\n",
        "\n",
        "                if not item_code:\n",
        "                    print(f\"⚠️ No se encontró item_code para serie '{serie}', omitiendo.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Verificar estado actual de la serie\n",
        "                    url_serie = f\"{FRAPPE_API}/Serial%20No/{serie}\"\n",
        "                    response_serie = session.get(url_serie, headers=HEADERS, timeout=10)\n",
        "\n",
        "                    if response_serie.status_code != 200:\n",
        "                        print(f\"⚠️ No se pudo verificar serie {serie}: {response_serie.status_code}\")\n",
        "                        continue\n",
        "\n",
        "                    serie_data = response_serie.json().get(\"data\", {})\n",
        "                    if serie_data.get(\"warehouse\") != origen:\n",
        "                        print(f\"⚠️ Serie {serie} no está en {origen}, está en {serie_data.get('warehouse')}\")\n",
        "                        continue\n",
        "\n",
        "                    # Crear la transferencia con enfoque alternativo para bundles\n",
        "                    stock_entry = {\n",
        "                        \"doctype\": \"Stock Entry\",\n",
        "                        \"stock_entry_type\": \"Material Transfer\",\n",
        "                        \"from_warehouse\": origen,\n",
        "                        \"to_warehouse\": destino,\n",
        "                        \"docstatus\": 1,\n",
        "                        \"items\": [{\n",
        "                            \"item_code\": item_code,\n",
        "                            \"qty\": 1,\n",
        "                            \"allow_zero_valuation_rate\": 1,\n",
        "                            \"use_serial_batch_fields\": 1,  # Evitar creación de nuevo bundle\n",
        "                            \"serial_no\": serie\n",
        "                        }]\n",
        "                    }\n",
        "\n",
        "                    # Primero intentar con serial_no\n",
        "                    response = session.post(\n",
        "                        url_frappe,\n",
        "                        headers=HEADERS,\n",
        "                        json=stock_entry,\n",
        "                        timeout=30\n",
        "                    )\n",
        "\n",
        "                    if response.status_code == 200:\n",
        "                        docname = response.json().get(\"data\", {}).get(\"name\")  # <- Capturamos el docname\n",
        "                        print(f\"✅ Transferido '{serie}' de '{origen}' a '{destino}'\")\n",
        "                        transferencias_por_tecnico[destino].append(f\"{serie} de {origen}\")\n",
        "                    else:\n",
        "                        error_msg = response.json().get(\"message\", response.text)\n",
        "                        print(f\"❌ Error al transferir '{serie}': {response.status_code} - {error_msg}\")\n",
        "\n",
        "                        # Si falla, intentar sin serial_no\n",
        "                        if \"ya está creado\" in error_msg:\n",
        "                            print(\"  ⚠️ Intentando solución alternativa sin serial_no...\")\n",
        "                            del stock_entry[\"items\"][0][\"serial_no\"]\n",
        "                            response_alt = session.post(\n",
        "                                url_frappe,\n",
        "                                headers=HEADERS,\n",
        "                                json=stock_entry,\n",
        "                                timeout=30\n",
        "                            )\n",
        "                            if response_alt.status_code == 200:\n",
        "                                print(f\"  ✅ Solución alternativa exitosa para '{serie}'\")\n",
        "                                transferencias_por_tecnico[destino].append(f\"{serie} de {origen}\")\n",
        "                            else:\n",
        "                                print(f\"  ❌ Falló solución alternativa: {response_alt.status_code} - {response_alt.text}\")\n",
        "\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    print(f\"❌ Error de conexión al transferir '{serie}': {str(e)}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error inesperado al transferir '{serie}': {str(e)}\")\n",
        "                agregar_comentario(\n",
        "                  doctype=\"Stock Entry\",\n",
        "                  docname=docname, # Reemplaza con un docname real de tu Frappe\n",
        "                  contenido=\"Transferencia realizada de forma automatica para una delivery note\"\n",
        "                )\n",
        "    print(\"✅ Transferencias completadas.\")\n",
        "    return transferencias_por_tecnico\n",
        "\n",
        "#transferirPendientes()"
      ],
      "metadata": {
        "id": "3Rjp9aT4vnZF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Santo Grial: Generación de la Nota de Entrega**\n",
        "\n",
        "* `deliveryNote(transferencias_por_tecnico)`: Esta es la función principal que crea las notas de entrega en Frappe. Agrupa todos los equipos ONT, decodificadores y consumibles por técnico, y luego genera un `Delivery Note` para cada uno. La función también agrega un comentario en el documento de Frappe que contiene una tabla HTML con los detalles de las Órdenes de Trabajo y las transferencias de stock que se realizaron previamente."
      ],
      "metadata": {
        "id": "cMUtmCWrv-WE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "6O_6aTXw7zaV"
      },
      "outputs": [],
      "source": [
        "def deliveryNote(transferencias_por_tecnico):\n",
        "    if hay_que_revisar():\n",
        "        #print(\"🚫 No se generó la nota de entrega. Revisá los archivos antes de continuar.\")\n",
        "        return\n",
        "\n",
        "    FRAPPE_API_delivery = f\"{FRAPPE_API}/Delivery%20Note\" # Use FRAPPE_API\n",
        "    HEADERS = {\n",
        "        \"Authorization\": f\"token {API_KEY}:{API_SECRET}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    cliente = \"Instalado\"\n",
        "    archivos_consumir = [archivo_ONT, archivo_DECO]\n",
        "\n",
        "    # Comentarios por técnico desde crudo\n",
        "    def comentarios_en_tabla_html():\n",
        "        comentarios = defaultdict(list)\n",
        "        try:\n",
        "            with open(archivo_crudo, newline='', encoding=\"utf-8\") as f:\n",
        "                reader = csv.reader(f)\n",
        "                # Skip header if present (assuming first row is header in original script)\n",
        "                # next(reader, None)\n",
        "                for fila in reader:\n",
        "                    if len(fila) >= 15:\n",
        "                        almacen = fila[3].strip()\n",
        "                        if not almacen:\n",
        "                            continue\n",
        "                        # Ensure cell content is HTML-safe if necessary (e.g., escape < > &)\n",
        "                        fila_html = \"\".join(f\"<td>{celda.strip()}</td>\" for celda in fila[:15])\n",
        "                        comentarios[almacen].append(f\"<tr>{fila_html}</tr>\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Archivo no encontrado: {archivo_crudo}\")\n",
        "        return comentarios\n",
        "\n",
        "    comentarios_por_almacen = comentarios_en_tabla_html()\n",
        "\n",
        "    # Diccionario de series\n",
        "    dic_series = {}\n",
        "    try:\n",
        "        with open(archivo_Series, newline='', encoding=\"utf-8\") as f_series:\n",
        "            reader = csv.reader(f_series)\n",
        "            next(reader) # Skip header\n",
        "            for fila in reader:\n",
        "                if len(fila) > 5:\n",
        "                    serie = fila[1].strip()\n",
        "                    item_code = fila[3].strip()\n",
        "                    item_name = fila[4].strip()\n",
        "                    dic_series[serie] = {\"item_code\": item_code, \"item_name\": item_name}\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Archivo no encontrado: {archivo_Series}\")\n",
        "\n",
        "    agrupados = defaultdict(lambda: defaultdict(lambda: {\"qty\": 0, \"seriales\": []}))\n",
        "    seriales_usados = set()\n",
        "\n",
        "    for archivo in archivos_consumir:\n",
        "        try:\n",
        "            with open(archivo, newline='', encoding=\"utf-8\") as f:\n",
        "                reader = csv.DictReader(f)\n",
        "                for fila in reader:\n",
        "                    serie = fila.get(\"Consumir\", \"\").strip()\n",
        "                    almacen = fila.get(\"Cargó OT\", \"\").strip()\n",
        "                    if not serie or not almacen:\n",
        "                        continue\n",
        "                    if serie in seriales_usados:\n",
        "                        print(f\"⚠️ Serie duplicada ignorada: {serie}\")\n",
        "                        continue\n",
        "                    if serie not in dic_series:\n",
        "                        print(f\"⚠️ Serie no encontrada en {archivo_Series}: {serie}\")\n",
        "                        continue\n",
        "                    info = dic_series[serie]\n",
        "                    key = (info[\"item_code\"], info[\"item_name\"])\n",
        "                    agrupados[almacen][key][\"qty\"] += 1\n",
        "                    agrupados[almacen][key][\"seriales\"].append(serie)\n",
        "                    seriales_usados.add(serie)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Archivo no encontrado: {archivo}\")\n",
        "\n",
        "    # Consumibles\n",
        "    consumibles_por_almacen = defaultdict(list)\n",
        "    try:\n",
        "        with open(archivo_consumibles, newline='', encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                item_code = row[\"item_code\"].strip()\n",
        "                item_name = row[\"item_name\"].strip()\n",
        "                warehouse = row[\"warehouse\"].strip()\n",
        "                try:\n",
        "                    qty = int(row[\"qty\"])\n",
        "                    if qty > 0:\n",
        "                        consumibles_por_almacen[warehouse].append({\n",
        "                            \"item_code\": item_code,\n",
        "                            \"item_name\": item_name,\n",
        "                            \"qty\": qty,\n",
        "                            \"warehouse\": warehouse\n",
        "                        })\n",
        "                except ValueError:\n",
        "                    print(f\"⚠️ Cantidad inválida en consumibles: {row['qty']} para item {item_name}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Archivo no encontrado: {archivo_consumibles}\")\n",
        "\n",
        "\n",
        "    # Crear delivery note por técnico\n",
        "    todos_los_almacenes = set(agrupados.keys()) | set(consumibles_por_almacen.keys())\n",
        "    for almacen in todos_los_almacenes:\n",
        "        items = []\n",
        "\n",
        "        for (item_code, item_name), datos in agrupados.get(almacen, {}).items():\n",
        "            items.append({\n",
        "                \"item_code\": item_code,\n",
        "                \"item_name\": item_name,\n",
        "                \"qty\": datos[\"qty\"],\n",
        "                \"warehouse\": almacen,\n",
        "                \"serial_no\": \"\\n\".join(datos[\"seriales\"])\n",
        "            })\n",
        "\n",
        "        for prod in consumibles_por_almacen.get(almacen, []):\n",
        "            items.append({\n",
        "                \"item_code\": prod[\"item_code\"],\n",
        "                \"item_name\": prod[\"item_name\"],\n",
        "                \"qty\": prod[\"qty\"],\n",
        "                \"warehouse\": almacen\n",
        "            })\n",
        "\n",
        "        if not items:\n",
        "            print(f\"ℹ️ No hay ítems para crear Delivery Note para el almacén: {almacen}\")\n",
        "            continue\n",
        "\n",
        "        payload = {\n",
        "            \"customer\": cliente,\n",
        "            \"posting_date\": str(date.today()),\n",
        "            \"set_warehouse\": almacen,\n",
        "            \"docstatus\": 1, # Validar DeliveryNote\n",
        "            \"items\": items,\n",
        "        }\n",
        "\n",
        "        print(f\"📦 Enviando nota de entrega para '{almacen}' con {len(items)} ítems...\")\n",
        "        response = requests.post(FRAPPE_API_delivery, headers=HEADERS, json=payload)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            docname = response.json()[\"data\"][\"name\"]\n",
        "            print(f\"✅ Nota de entrega creada: {FRAPPE_API_crudo}/app/delivery-note/{docname}\")\n",
        "\n",
        "            # Paso 3: comentario con datos de crudo\n",
        "            filas_html_list = comentarios_por_almacen.get(almacen, [])\n",
        "            if filas_html_list:\n",
        "                full_comment_html = \"<h3>Detalle del Consumo (Crudo):</h3>\"\n",
        "                full_comment_html += \"<table border='1' style='width:100%; border-collapse: collapse;'>\"\n",
        "                full_comment_html += \"<thead><tr>\" + \"\".join([f\"<th>Col{i+1}</th>\" for i in range(15)]) + \"</tr></thead>\" # Add a simple header for clarity\n",
        "                full_comment_html += \"<tbody>\"\n",
        "                full_comment_html += \"\".join(filas_html_list) # Join the list of <tr>...</tr> strings\n",
        "                full_comment_html += \"</tbody></table>\"\n",
        "\n",
        "                agregar_comentario(\n",
        "                  doctype=\"Delivery Note\",\n",
        "                  docname=docname,\n",
        "                  contenido=full_comment_html\n",
        "                )\n",
        "\n",
        "            # Paso 4: comentario con transferencias realizadas a este técnico\n",
        "            transferencias_list = transferencias_por_tecnico.get(almacen)\n",
        "            if transferencias_list:\n",
        "                transfer_comment_html = \"<h3>Transferencias Relacionadas:</h3>\"\n",
        "                transfer_comment_html += \"<table border='1' style='width:100%; border-collapse: collapse;'>\"\n",
        "                transfer_comment_html += \"<tbody>\"\n",
        "                transfer_comment_html += \"🚚 \".join([f\"<tr><td>{t}</td></tr>\" for t in transferencias_list]) # Join the list of <tr>...</tr> strings\n",
        "                transfer_comment_html += \"</tbody></table>\"\n",
        "\n",
        "                agregar_comentario(\n",
        "                  doctype=\"Delivery Note\",\n",
        "                  docname=docname,\n",
        "                  contenido=transfer_comment_html\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            print(f\"❌ Error al crear nota de entrega para {almacen}:\")\n",
        "            print(response.status_code, response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OCR"
      ],
      "metadata": {
        "id": "GqOiY8nOwQFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "19crc6NgwR_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Descarga de imágenes**\n",
        "\n",
        "* `extraer_file_id_google_drive(url)`: Esta función extrae el ID único de un archivo de Google Drive a partir de su URL. Es una función de utilidad que permite al script identificar la imagen que necesita descargar.\n",
        "* `download_image_from_drive(file_id, dest_path)`: Utiliza el ID del archivo de Google Drive para descargar la imagen y la guarda en una ruta local. También realiza una verificación básica para asegurarse de que el archivo descargado sea realmente una imagen válida.\n",
        "* `extraer_links_unicos_de_csvs(csvs, columna=\"foto (no recibido)\")`: Recorre los archivos CSV de ONT y Decos para encontrar todas las URLs de las fotos de los equipos que no fueron recibidos. Devuelve una lista de URLs únicas y un diccionario que mapea cada URL con el número de serie correspondiente, para saber qué imagen pertenece a qué equipo."
      ],
      "metadata": {
        "id": "4POZ5l_WzMZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_file_id_google_drive(url):\n",
        "    for patron in [r'id=([a-zA-Z0-9_\\-]+)', r'/d/([a-zA-Z0-9_\\-]+)']:\n",
        "        m = re.search(patron, url)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    return None\n",
        "\n",
        "def download_image_from_drive(file_id, dest_path):\n",
        "    url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "    r = requests.get(url)\n",
        "    if r.status_code == 200:\n",
        "        with open(dest_path, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "\n",
        "        # Validar si es imagen válida\n",
        "        try:\n",
        "            img = cv2.imread(dest_path)\n",
        "            if img is None:\n",
        "                print(f\"⚠️ Archivo no es una imagen válida: {file_id}\")\n",
        "                os.remove(dest_path)\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error leyendo imagen: {file_id} - {e}\")\n",
        "            os.remove(dest_path)\n",
        "            return False\n",
        "\n",
        "        print(f\"📥 Imagen descargada en {dest_path}\")\n",
        "        return True\n",
        "\n",
        "    print(f\"❌ Error al descargar {file_id} (HTTP {r.status_code})\")\n",
        "    return False\n",
        "\n",
        "\n",
        "def extraer_links_unicos_de_csvs(csvs, columna=\"foto (no recibido)\"):\n",
        "    enlaces_unicos = set()\n",
        "    mapeo_url_a_serie = {}\n",
        "\n",
        "    for path in csvs:\n",
        "        with open(path, encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            headers = reader.fieldnames\n",
        "\n",
        "            # Detectar columna de serie (puede variar entre ONT o Deco)\n",
        "            posibles_series = [\"serie\", \"ONT (modem fibra)\", \"Deco\", \"Consumir\", \"No recibido\"]\n",
        "            col_serie = next((c for c in posibles_series if c in headers), None)\n",
        "\n",
        "            if not col_serie:\n",
        "                continue\n",
        "\n",
        "            for i, fila in enumerate(reader):\n",
        "                origen = fila.get(\"Origen equipo\", \"\").strip().lower()\n",
        "                if origen != \"no\":\n",
        "                    continue\n",
        "\n",
        "                serie = fila.get(col_serie, \"\").strip().upper()\n",
        "                raw_links = fila.get(columna, \"\")\n",
        "\n",
        "                if not serie:\n",
        "                    continue\n",
        "                if not raw_links:\n",
        "                    continue\n",
        "\n",
        "                for url in raw_links.splitlines():\n",
        "                    url = url.strip()\n",
        "                    if url.startswith(\"http\"):\n",
        "                        enlaces_unicos.add(url)\n",
        "                        mapeo_url_a_serie[url] = serie\n",
        "\n",
        "    return list(enlaces_unicos), mapeo_url_a_serie"
      ],
      "metadata": {
        "id": "M-g_IjeEzLfE"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Rotación de las imagenes**\n",
        "\n",
        "* `rotar_imagen_cv2(img, angle)`: Esta es una función de utilidad que rota una imagen en un ángulo específico (90, 180 o 270 grados) usando la biblioteca `cv2`.\n",
        "* `asegurar_apaisado(img)`: Rota automáticamente una imagen a una orientación horizontal si es que esta se encuentra en una orientación vertical.\n",
        "* `ocr_en_varias_rotaciones(img_path, ocr_detector, prefijos_validos)`: Esta es la función principal de OCR. Realiza un reconocimiento de texto en varias rotaciones de una imagen (0 y 180 grados) para asegurar una lectura correcta. Da prioridad a la rotación que encuentre un número de serie con uno de los `prefijos_validos`. Si no encuentra ningún prefijo válido, devuelve el resultado de la rotación que haya detectado más texto como un intento de respaldo."
      ],
      "metadata": {
        "id": "C1YpjxYezUup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rotar_imagen_cv2(img, angle):\n",
        "    if angle == 90 or angle == 270:\n",
        "        return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE if angle == 90 else cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
        "    elif angle == 180:\n",
        "        return cv2.rotate(img, cv2.ROTATE_180)\n",
        "    return img\n",
        "\n",
        "def asegurar_apaisado(img):\n",
        "    h, w = img.shape[:2]\n",
        "    return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE) if h > w else img\n",
        "\n",
        "\n",
        "def ocr_en_varias_rotaciones(img_path, ocr_detector, prefijos_validos):\n",
        "    \"\"\"\n",
        "    Realiza OCR en varias rotaciones de una imagen, priorizando la rotación\n",
        "    que contenga un texto con un prefijo válido.\n",
        "    \"\"\"\n",
        "    img = asegurar_apaisado(cv2.imread(img_path))\n",
        "    mejor_resultado = None\n",
        "    mejor_resultado_con_prefijo = None\n",
        "    max_textos = 0\n",
        "\n",
        "    for angle in [0, 180]:\n",
        "        print(f\"\\n🔄 Rotación {angle}°\")\n",
        "        rotada = rotar_imagen_cv2(img, angle)\n",
        "        temp = \"/tmp/temp_ocr.jpg\"\n",
        "        cv2.imwrite(temp, rotada)\n",
        "        resultado = ocr_detector.predict(temp)\n",
        "        os.remove(temp)\n",
        "\n",
        "        if not resultado:\n",
        "            continue\n",
        "\n",
        "        textos = resultado[0].get('rec_texts', [])\n",
        "        print(\"📌 Textos:\", textos)\n",
        "\n",
        "        # --- Nueva lógica de selección ---\n",
        "        contiene_prefijo = False\n",
        "        texto_con_prefijo = None\n",
        "        for t in textos:\n",
        "            texto_limpio = t.strip().upper()\n",
        "            if any(texto_limpio.startswith(p) for p in prefijos_validos):\n",
        "                contiene_prefijo = True\n",
        "                texto_con_prefijo = t  # Guarda el texto completo que contiene el prefijo\n",
        "                break\n",
        "\n",
        "        # Si se encuentra un prefijo válido en esta rotación, la guardamos como la mejor.\n",
        "        if contiene_prefijo:\n",
        "            mejor_resultado_con_prefijo = resultado\n",
        "            # No necesitamos seguir buscando, esta es la mejor opción.\n",
        "            break\n",
        "\n",
        "        # Si no hay prefijo válido, nos quedamos con la rotación que tenga más texto\n",
        "        # para tener un respaldo en caso de que no haya coincidencias de prefijos en ninguna rotación.\n",
        "        if len(textos) > max_textos:\n",
        "            mejor_resultado = resultado\n",
        "            max_textos = len(textos)\n",
        "\n",
        "    # Si encontramos un resultado con un prefijo válido, lo retornamos\n",
        "    if mejor_resultado_con_prefijo:\n",
        "        print(\"✅ Se encontró una rotación con prefijo válido. Retornando el resultado.\")\n",
        "        return mejor_resultado_con_prefijo\n",
        "\n",
        "    # Si no, retornamos el resultado que tenga la mayor cantidad de texto\n",
        "    if mejor_resultado:\n",
        "        print(\"ℹ️ No se encontraron prefijos válidos. Retornando el resultado con más texto.\")\n",
        "    else:\n",
        "        print(\"❌ No se detectó ningún texto en ninguna rotación.\")\n",
        "\n",
        "    return mejor_resultado"
      ],
      "metadata": {
        "id": "XkLPbnQdzZEy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Filtrado y comparación de series**\n",
        "\n",
        "* `filtrar_numeros_serie(resultados)`: Esta función toma los resultados del OCR y los filtra para encontrar números de serie válidos. Busca cadenas de texto que comiencen con cualquiera de los `prefijos_validos` y se asegura de que la cadena resultante tenga al menos 6 caracteres y sea alfanumérica.\n",
        "* `cargar_series_no_recibidas(csvs)`: Lee los archivos CSV y extrae una lista de tuplas, donde cada tupla contiene el número de serie original y la URL de la foto del equipo que no fue recibido. Esto prepara los datos para el proceso de corrección.\n",
        "* `procesar_fotos_con_ocr(...)`: Esta es la función principal que coordina el proceso de OCR. Primero, consulta la API de Frappe para obtener la lista completa de números de serie válidos. Luego, descarga las imágenes de los equipos no recibidos y ejecuta el OCR en cada una. Finalmente, compara las series detectadas por el OCR con la base de datos de Frappe para validar las correcciones y las exporta a un archivo CSV.\n",
        "* `run_ocr_si_hay_que_revisar(...)`: Actúa como la función de entrada para el proceso de OCR. Llama a `extraer_links_unicos_de_csvs` para obtener las URLs de las fotos de los equipos no recibidos y luego a `procesar_fotos_con_ocr` para ejecutar el reconocimiento.\n",
        "* `exportar_series_detectadas_csv(...)`: Genera un archivo CSV que mapea cada serie original (la que no fue recibida) con la serie corregida que fue detectada y validada por el OCR, facilitando la corrección manual o automática.\n",
        "* `obtener_mapeo_file_id_a_idx(no_recibidos)` y `exportar_url_serie_a_csv(...)`: Funciones de utilidad para manejar la relación entre los IDs de archivo, las URLs y los números de serie originales.\n",
        "* `comparar_series_viejos_y_nuevos(...)`: Compara las series originales con las series corregidas del OCR para identificar posibles errores. Utiliza el algoritmo de distancia de Levenshtein para sugerir las correcciones más probables, lo que ayuda a los usuarios a validar los resultados del OCR."
      ],
      "metadata": {
        "id": "uYVVpS4ezuXz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "kkOCc4LKAqlJ"
      },
      "outputs": [],
      "source": [
        "def filtrar_numeros_serie(resultados):\n",
        "    \"\"\"\n",
        "    Filtra los números de serie de los resultados del OCR.\n",
        "    La lógica se basa en buscar prefijos válidos dentro de la cadena de texto\n",
        "    y extraer la parte del texto que comienza con ese prefijo.\n",
        "\n",
        "    Args:\n",
        "        resultados (list): Una lista de resultados de OCR, donde cada elemento es un diccionario\n",
        "                         con una clave 'rec_texts' que contiene una lista de strings.\n",
        "\n",
        "    Returns:\n",
        "        list: Una lista de strings únicos que son números de serie válidos.\n",
        "    \"\"\"\n",
        "    encontrados = set()\n",
        "\n",
        "    for bloque in resultados:\n",
        "        for txt in bloque.get('rec_texts', []):\n",
        "            t = txt.strip().upper()\n",
        "\n",
        "            for prefijo in prefijos_validos:\n",
        "                if prefijo in t:\n",
        "                    inicio = t.find(prefijo)\n",
        "\n",
        "                    numero_serie_candidato = t[inicio:]\n",
        "\n",
        "                    if len(numero_serie_candidato) >= 6 and numero_serie_candidato.isalnum():\n",
        "                        encontrados.add(numero_serie_candidato)\n",
        "                        break\n",
        "\n",
        "    return list(encontrados)\n",
        "\n",
        "\n",
        "def cargar_series_no_recibidas(csvs):\n",
        "    \"\"\"\n",
        "    Retorna una lista de tuplas: (serie_original, link_individual)\n",
        "    Si hay más de un link en la celda, los divide correctamente.\n",
        "    \"\"\"\n",
        "    pares = []\n",
        "    for path in csvs:\n",
        "        with open(path, encoding=\"utf-8\") as f:\n",
        "            for fila in csv.DictReader(f):\n",
        "                serie = fila.get(\"serie\", \"\").strip().upper()\n",
        "                enlaces = fila.get(\"foto (no recibido)\", \"\").strip()\n",
        "                if not serie or not enlaces:\n",
        "                    continue\n",
        "                for link in enlaces.splitlines():\n",
        "                    if link.strip().startswith(\"http\"):\n",
        "                        pares.append((serie, link.strip()))\n",
        "    return pares\n",
        "\n",
        "\n",
        "def procesar_fotos_con_ocr(enlaces, ocr_detector, api_key, api_secret, frappe_api, no_recibidos=None):\n",
        "    print(\"\\n🔌 Obteniendo series del ERP...\")\n",
        "    try:\n",
        "        headers = {\"Authorization\": f\"token {api_key}:{api_secret}\"}\n",
        "        url = f\"{frappe_api}/Serial%20No?fields=[\\\"name\\\"]&limit_page_length=1000\"\n",
        "        data = requests.get(url, headers=headers).json().get(\"data\", [])\n",
        "        series_erp = set(x[\"name\"].upper() for x in data if x.get(\"name\"))\n",
        "        print(f\"✅ {len(series_erp)} series obtenidas\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error ERP: {e}\")\n",
        "        return {}\n",
        "\n",
        "    resultados, mapeos = {}, []\n",
        "    filas_validas = []\n",
        "\n",
        "    for i, url in enumerate(enlaces):\n",
        "        print(f\"\\n📷 Imagen {i+1}/{len(enlaces)}: {url}\")\n",
        "        file_id = extraer_file_id_google_drive(url)\n",
        "        if not file_id:\n",
        "            print(\"⚠️ No se pudo extraer ID\")\n",
        "            continue\n",
        "\n",
        "        path = f\"/tmp/imagen_{i}.jpg\"\n",
        "\n",
        "        if not download_image_from_drive(file_id, path):\n",
        "            continue\n",
        "\n",
        "        ocr = ocr_en_varias_rotaciones(path, ocr_detector, prefijos_validos)\n",
        "        series_detectadas = filtrar_numeros_serie(ocr or [])\n",
        "        verificacion = {s: s in series_erp for s in series_detectadas}\n",
        "\n",
        "        print(\"🔎 Series detectadas:\", series_detectadas)\n",
        "        print(\"✅ Validación:\", verificacion)\n",
        "\n",
        "        for s in series_detectadas:\n",
        "            if verificacion.get(s):\n",
        "                filas_validas.append((url, s))\n",
        "\n",
        "        if no_recibidos:\n",
        "            for detectada in series_detectadas:\n",
        "                for original, link in no_recibidos.items():\n",
        "                    if file_id in link and detectada != original:\n",
        "                        mapeos.append(f\"{original} -> {detectada}\")\n",
        "\n",
        "        resultados[url] = {\n",
        "            \"series_detectadas\": series_detectadas,\n",
        "            \"verificacion\": verificacion\n",
        "        }\n",
        "\n",
        "        os.remove(path)\n",
        "\n",
        "    if filas_validas:\n",
        "        with open(path_nuevos, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"url\", \"serie\"])\n",
        "            writer.writerows(filas_validas)\n",
        "        print(f\"\\n✅ CSV generado: series_validas_detectadas.csv ({len(filas_validas)} filas)\")\n",
        "    else:\n",
        "        print(\"\\n⚠️ No se detectaron series válidas, no se generó CSV.\")\n",
        "\n",
        "    return {\n",
        "        \"resultados\": resultados,\n",
        "        \"mapeos_sugeridos\": mapeos\n",
        "    }\n",
        "\n",
        "\n",
        "def run_ocr_si_hay_que_revisar(\n",
        "    ocr_detector,\n",
        "    api_key,\n",
        "    api_secret,\n",
        "    frappe_api,\n",
        "    archivo_ONT,\n",
        "    archivo_DECO\n",
        "):\n",
        "    print(\"⚠️ Ejecutando OCR por no recibidos...\")\n",
        "    enlaces = extraer_links_unicos_de_csvs([archivo_ONT, archivo_DECO])[0]\n",
        "    print(f\"🔗 {len(enlaces)} enlaces únicos\")\n",
        "\n",
        "    no_recibidos = cargar_series_no_recibidas([archivo_ONT, archivo_DECO])\n",
        "    resultado = procesar_fotos_con_ocr(enlaces, ocr_detector, api_key, api_secret, frappe_api, no_recibidos)\n",
        "\n",
        "    for mapeo in resultado[\"mapeos_sugeridos\"]:\n",
        "        print(\"🔁\", mapeo)\n",
        "\n",
        "    return resultado\n",
        "\n",
        "def exportar_series_detectadas_csv(\n",
        "    resultados_ocr: dict,\n",
        "    url_a_serie_original: dict,\n",
        "    path_salida=\"series_detectadas_con_validacion.csv\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Exporta un CSV con:\n",
        "    - serie original (de los no recibidos)\n",
        "    - serie corregida detectada y validada por OCR\n",
        "\n",
        "    Usa la URL como clave para relacionar datos.\n",
        "    \"\"\"\n",
        "\n",
        "    filas = []\n",
        "\n",
        "    for url, serie_original in url_a_serie_original.items():\n",
        "        resultado = resultados_ocr.get(url)\n",
        "        if not resultado:\n",
        "            continue\n",
        "        if \"verificacion\" not in resultado:\n",
        "            continue\n",
        "\n",
        "        # Filtrar solo series validadas True\n",
        "        series_validas = [s for s, ok in resultado[\"verificacion\"].items() if ok]\n",
        "\n",
        "        # Si hay series válidas, hacer una fila por cada una\n",
        "        for serie_corregida in series_validas:\n",
        "            filas.append([serie_original, serie_corregida])\n",
        "\n",
        "    # Guardar CSV con SOLO dos columnas: serie original y serie corregida\n",
        "    with open(path_salida, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['serie_original', 'serie_corregida_valida'])\n",
        "        writer.writerows(filas)\n",
        "\n",
        "    print(f\"✅ CSV generado: {path_salida} ({len(filas)} filas)\")\n",
        "\n",
        "\n",
        "\n",
        "def obtener_mapeo_file_id_a_idx(no_recibidos):\n",
        "    \"\"\"\n",
        "    Devuelve un dict: file_id => índice en la lista de no_recibidos\n",
        "    \"\"\"\n",
        "    mapeo = {}\n",
        "    for idx, (_, url) in enumerate(no_recibidos):\n",
        "        file_id = extraer_file_id_google_drive(url)\n",
        "        if file_id:\n",
        "            mapeo[file_id] = idx\n",
        "    return mapeo\n",
        "\n",
        "def exportar_url_serie_a_csv(url_a_serie_dict, path_salida=path_viejos):\n",
        "    \"\"\"\n",
        "    Exporta un CSV con dos columnas:\n",
        "    url, serie\n",
        "    a partir de un diccionario {url: serie}\n",
        "    \"\"\"\n",
        "    filas = [(url, serie) for url, serie in url_a_serie_dict.items()]\n",
        "\n",
        "    with open(path_salida, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['url', 'serie'])\n",
        "        writer.writerows(filas)\n",
        "\n",
        "    print(f\"✅ CSV generado: {path_salida} ({len(filas)} filas)\")\n",
        "\n",
        "\n",
        "def comparar_series_viejos_y_nuevos(\n",
        "    path_viejos=path_viejos,\n",
        "    path_nuevos=path_nuevos\n",
        "):\n",
        "    # Verificar existencia de archivos\n",
        "    if not os.path.exists(path_viejos):\n",
        "        print(f\"❌ Archivo no encontrado: {path_viejos}\")\n",
        "        return\n",
        "    elif not os.path.exists(path_nuevos):\n",
        "        print(f\"❌ Archivo no encontrado: {path_nuevos}\")\n",
        "        return\n",
        "\n",
        "    def leer_csv(path):\n",
        "        d = {}\n",
        "        with open(path, encoding=\"utf-8\") as f:\n",
        "            for row in csv.DictReader(f):\n",
        "                url = row[\"url\"].strip()\n",
        "                serie = row[\"serie\"].strip().upper()\n",
        "                d.setdefault(url, []).append(serie)\n",
        "        return d\n",
        "\n",
        "    def levenshtein(a, b):\n",
        "        if len(a) < len(b):\n",
        "            return levenshtein(b, a)\n",
        "        if len(b) == 0:\n",
        "            return len(a)\n",
        "        previous_row = range(len(b) + 1)\n",
        "        for i, c1 in enumerate(a):\n",
        "            current_row = [i + 1]\n",
        "            for j, c2 in enumerate(b):\n",
        "                insertions = previous_row[j + 1] + 1\n",
        "                deletions = current_row[j] + 1\n",
        "                substitutions = previous_row[j] + (c1 != c2)\n",
        "                current_row.append(min(insertions, deletions, substitutions))\n",
        "            previous_row = current_row\n",
        "        return previous_row[-1]\n",
        "\n",
        "    def sugerir_correccion(serie_vieja, series_validas):\n",
        "        if not series_validas:\n",
        "            return \"\"\n",
        "        similitudes = [(s, levenshtein(serie_vieja, s)) for s in series_validas]\n",
        "        similitudes.sort(key=lambda x: x[1])\n",
        "        return similitudes[0][0]  # menor distancia\n",
        "\n",
        "    viejos = leer_csv(path_viejos)\n",
        "    nuevos = leer_csv(path_nuevos)\n",
        "    todas_las_urls = sorted(set(viejos) | set(nuevos))\n",
        "\n",
        "    print(\"\\n📊 Comparación de series:\")\n",
        "    for url in todas_las_urls:\n",
        "        viejas = viejos.get(url, [])\n",
        "        nuevas = nuevos.get(url, [])\n",
        "        print(f\"{url} - {viejas} -> {nuevas}\")\n",
        "\n",
        "    print(\"\\n🔍 Sugerencias automáticas de corrección:\")\n",
        "    for url in todas_las_urls:\n",
        "        viejas = viejos.get(url, [])\n",
        "        nuevas = nuevos.get(url, [])\n",
        "        for vieja in viejas:\n",
        "            sugerida = sugerir_correccion(vieja, nuevas)\n",
        "            if sugerida and sugerida != vieja:\n",
        "                print(f\"🔁 {url} - Corregir '{vieja}' -> '{sugerida}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Función principal (`main`)**\n",
        "\n",
        "`main()`: Esta es la función principal que orquesta todo el flujo de trabajo del script. Ejecuta las siguientes etapas de forma secuencial:\n",
        "1.  **Limpieza inicial**: Elimina archivos temporales de ejecuciones anteriores.\n",
        "2.  **Obtención de datos**: Llama a las funciones para descargar la planilla de consumo y los datos de stock y series de Frappe.\n",
        "3.  **Filtrado y procesamiento**: Procesa los datos para extraer la información de equipos ONT, decodificadores y consumibles.\n",
        "4.  **Transferencias**: Revisa si hay equipos que necesitan ser transferidos entre técnicos y realiza las transferencias necesarias en el ERP.\n",
        "5.  **Generación de notas de entrega**: Crea las notas de entrega (`Delivery Notes`) en Frappe, agrupando los ítems por técnico y agregando comentarios detallados.\n",
        "6.  **OCR condicional**: Si aún quedan equipos marcados como \"no recibidos\", inicia un proceso de OCR (Reconocimiento Óptico de Caracteres). Este subproceso instala las librerías necesarias, descarga las fotos, las procesa para leer los números de serie, los valida con el ERP y genera un reporte de correcciones sugeridas.\n",
        "7.  **Limpieza final**: Después de completar el flujo principal o el OCR, limpia los archivos CSV temporales."
      ],
      "metadata": {
        "id": "MsNl0kzJ3CZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    #os.system(\"clear\") #quitar comentario en caso de usar en local, para mas estetica\n",
        "\n",
        "    # Limpieza previa\n",
        "    borrar_archivos_csv() #quitar comentario en caso de usar en local\n",
        "\n",
        "    # Obtención de datos\n",
        "    obtenerCrudo()\n",
        "    obtenerSeriesFrappe()\n",
        "    obtenerProductos()\n",
        "\n",
        "    # Procesamiento\n",
        "    extraer_columnas_ont()\n",
        "    extraer_columnas_deco()\n",
        "    extraer_consumibles()\n",
        "\n",
        "    # Transferencias\n",
        "    transferencias_por_tecnico = transferirPendientes()\n",
        "\n",
        "    # Si tiene seguro que abre, sino no\n",
        "    #abrirFotosNoRecibidos() # en el caso de colab no hace falta jsajdsja\n",
        "\n",
        "    # Deliverys\n",
        "    print(\"\\n\\n--- Generando Delivery Note ---\")\n",
        "    deliveryNote(transferencias_por_tecnico)\n",
        "\n",
        "    # Ahora chequeo si hay que revisar con OCR:\n",
        "    if hay_que_revisar(only_boolean=True):\n",
        "        print(\"\\n--- Revisando dependencias para comenzar con el OCR ---\")\n",
        "\n",
        "        \"\"\"OCR batch desde enlaces de Google Drive con rotación automática\"\"\"\n",
        "        # Instalaciones de dependencias de ocr(solo en Colab)\n",
        "        # solo si hace falta\n",
        "        !pip install pytesseract\n",
        "        !apt-get install -y tesseract-ocr libtesseract-dev\n",
        "        if gpu_o_cpu==\"C\" or gpu_o_cpu == \"\":\n",
        "          !pip install paddlepaddle -f https://www.paddlepaddle.org.cn/whl/quick_install.html # en caso de usar local para poder usar el procesamiento a través de CPU y no de grafica, tarda mas solamente\n",
        "        if gpu_o_cpu.upper() == \"G\":\n",
        "          !pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu118/\n",
        "        !pip install paddleocr\n",
        "\n",
        "        from paddleocr import PaddleOCR\n",
        "\n",
        "        print(\"\\n--- Lanzando OCR por equipos no recibidos ---\")\n",
        "\n",
        "        ocr_detector = PaddleOCR(det_model_dir=None, use_textline_orientation=True, lang='en')\n",
        "\n",
        "        resultado_ocr = run_ocr_si_hay_que_revisar(\n",
        "            ocr_detector,\n",
        "            API_KEY,\n",
        "            API_SECRET,\n",
        "            FRAPPE_API,\n",
        "            archivo_ONT,\n",
        "            archivo_DECO\n",
        "        )\n",
        "\n",
        "        # Cargar no_recibidos como lista de tuplas (serie_original, url)\n",
        "        no_recibidos = cargar_series_no_recibidas([archivo_ONT, archivo_DECO])\n",
        "\n",
        "        # Convertir a dict {url: serie_original}\n",
        "        url_a_serie_original = {url: serie for serie, url in no_recibidos}\n",
        "\n",
        "        # series mal\n",
        "        url_a_serie = extraer_links_unicos_de_csvs([archivo_ONT, archivo_DECO])[1]\n",
        "        exportar_url_serie_a_csv(url_a_serie)\n",
        "\n",
        "        comparar_series_viejos_y_nuevos()\n",
        "        borrar_archivos_csv()\n",
        "\n",
        "    else:\n",
        "        print(\"\\n--- Todos los equipos recibidos, seguimos con flujo normal ---\")\n",
        "        ultimaOTconsumida()\n",
        "        borrar_archivos_csv()  # sacar el comentario en caso de usar local para que se puedan actualizar los archivos descargados\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "PDVRh5b9BEMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b0409c-64ce-44cc-be00-0ab39b18881b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧹 Archivo eliminado: crudo.csv\n",
            "🧹 Archivo eliminado: series_frappe.csv\n",
            "🧹 Archivo eliminado: productos.csv\n",
            "🧹 Archivo eliminado: columnas_extraidasONT.csv\n",
            "🧹 Archivo eliminado: columnas_extraidasDeco.csv\n",
            "🧹 Archivo eliminado: reporte_consumibles.csv\n",
            "Descargando CSV...\n",
            "Filtrando filas que no hayan sido consumidas...\n",
            "✅ Filtrado completo. 30 filas guardadas en 'crudo.csv'\n",
            "Consultando series en Frappe...\n",
            "✅ 1000 series guardadas en 'series_frappe.csv'\n",
            "Iniciando descarga de stock proyectado...\n",
            "Se obtuvieron 104 registros de stock\n",
            "✅ Datos guardados correctamente en productos.csv\n",
            "✅ Archivo 'columnas_extraidasONT.csv' generado con 21 filas filtradas.\n",
            "✅ Archivo 'columnas_extraidasDeco.csv' generado con 19 filas.\n",
            "🛠️ Iniciando extracción y cálculo de consumibles...\n",
            "⚠️ Valor no numérico en columna O de 'crudo.csv' para 51ROD636 (Pat. AD833WB) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "⚠️ Valor no numérico en columna O de 'crudo.csv' para 51ROD637 (Pat. AC774WF) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "⚠️ Valor no numérico en columna O de 'crudo.csv' para 51ROD637 (Pat. AC774WF) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "⚠️ Valor no numérico en columna O de 'crudo.csv' para 51ROD637 (Pat. AC774WF) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "⚠️ Valor no numérico en columna O de 'crudo.csv' para 51ROD636 (Pat. AD833WB) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "⚠️ Valor no numérico en columna O de 'crudo.csv' para 51ROD636 (Pat. AD833WB) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "⚠️ Valor no numérico en columna O de 'crudo.csv' para 51ROD465 (Pat. AE835PS) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "⚠️ Valor no numérico en columna O de 'crudo.csv' para 51ROD465 (Pat. AE835PS) - QPS: '--------SIN UTILIZAR--------'. Ignorado.\n",
            "\n",
            "🔄 Procesando ítems de 'consumir todo el stock'...\n",
            "✅ Consumiendo 100 de 170300004 en 51ROD465 (Pat. AE835PS) - QPS (ítem de consumo total).\n",
            "✅ Archivo 'reporte_consumibles.csv' generado con 21 filas (consumos ajustados por stock y lista ITEMS_CONSUMIR_TODO_STOCK).\n",
            "✨ Proceso de extracción y cálculo de consumibles completado.\n",
            "🚚 Procesando transferencias pendientes...\n",
            "✅ Transferido 'FZ324060208468' de '51ROD465 (Pat. AE835PS) - QPS' a '51ROD636 (Pat. AD833WB) - QPS'\n",
            "🗨️ Comentario agregado a MAT-STE-2025-01380 (Stock Entry) usando add_comment\n",
            "✅ Transferencias completadas.\n",
            "\n",
            "\n",
            "--- Generando Delivery Note ---\n",
            "📦 Enviando nota de entrega para '51ROD637 (Pat. AC774WF) - QPS' con 8 ítems...\n",
            "✅ Nota de entrega creada: https://ad3b02a9cc80.ngrok-free.app/app/delivery-note/MAT-DN-2025-00143\n",
            "🗨️ Comentario agregado a MAT-DN-2025-00143 (Delivery Note) usando add_comment\n",
            "📦 Enviando nota de entrega para '51ROD636 (Pat. AD833WB) - QPS' con 8 ítems...\n",
            "✅ Nota de entrega creada: https://ad3b02a9cc80.ngrok-free.app/app/delivery-note/MAT-DN-2025-00144\n",
            "🗨️ Comentario agregado a MAT-DN-2025-00144 (Delivery Note) usando add_comment\n",
            "🗨️ Comentario agregado a MAT-DN-2025-00144 (Delivery Note) usando add_comment\n",
            "📦 Enviando nota de entrega para '51ROD465 (Pat. AE835PS) - QPS' con 9 ítems...\n",
            "✅ Nota de entrega creada: https://ad3b02a9cc80.ngrok-free.app/app/delivery-note/MAT-DN-2025-00145\n",
            "🗨️ Comentario agregado a MAT-DN-2025-00145 (Delivery Note) usando add_comment\n",
            "\n",
            "--- Todos los equipos recibidos, seguimos con flujo normal ---\n",
            "\n",
            "\n",
            "Última OT del crudo.csv: SA489369\n",
            "Enviado a Forms\n",
            "\n",
            "\n",
            "🧹 Archivo eliminado: crudo.csv\n",
            "🧹 Archivo eliminado: series_frappe.csv\n",
            "🧹 Archivo eliminado: productos.csv\n",
            "🧹 Archivo eliminado: columnas_extraidasONT.csv\n",
            "🧹 Archivo eliminado: columnas_extraidasDeco.csv\n",
            "🧹 Archivo eliminado: reporte_consumibles.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNQYBzdC52Db4+wJP9xgybz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}